{"cells":[{"cell_type":"markdown","metadata":{"id":"4bKQIsIq-d8y"},"source":["#**Llama 2**"]},{"cell_type":"markdown","metadata":{"id":"PnV5UC7A2vBZ"},"source":["The Llama 2 is a collection of pretrained and fine-tuned generative text models, ranging from 7 billion to 70 billion parameters, designed for dialogue use cases."]},{"cell_type":"markdown","metadata":{"id":"AC41zK5l3Abp"},"source":[" It outperforms open-source chat models on most benchmarks and is on par with popular closed-source models in human evaluations for helpfulness and safety."]},{"cell_type":"markdown","metadata":{"id":"4nobX9E83PjQ"},"source":["[Llama 2 13B-chat](https://huggingface.co/meta-llama/Llama-2-13b-chat)"]},{"cell_type":"markdown","metadata":{"id":"3YC846SH5DOK"},"source":["#  Quantized Models from the Hugging Face Community"]},{"cell_type":"markdown","metadata":{"id":"0TD82wis5LGA"},"source":["The Hugging Face community provides quantized models, which allow us to efficiently and effectively utilize the model on the T4 GPU.\n","\n","There are several variations available but we are using the model called [Llama-2-13B-chat-GGML](https://huggingface.co/TheBloke/Llama-2-13B-chat-GGML)."]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ps94s2Tkngqi","executionInfo":{"status":"ok","timestamp":1698838263509,"user_tz":-330,"elapsed":26692,"user":{"displayName":"Prakhar Singh","userId":"07600268745645597584"}},"outputId":"90116289-2e9d-41ac-da10-3741233e928b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"markdown","metadata":{"id":"YQZBmz7I5neU"},"source":["#**Step 1: Installation of all the Required Packages**"]},{"cell_type":"code","execution_count":1,"metadata":{"id":"L0avf7xx2lcj","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1705600671835,"user_tz":-330,"elapsed":131488,"user":{"displayName":"Prakhar Singh","userId":"07600268745645597584"}},"outputId":"2d31ae2a-f1e3-45f6-d34d-5f2807787522"},"outputs":[{"output_type":"stream","name":"stdout","text":["Using pip 23.1.2 from /usr/local/lib/python3.10/dist-packages/pip (python 3.10)\n","Collecting llama-cpp-python==0.1.78\n","  Downloading llama_cpp_python-0.1.78.tar.gz (1.7 MB)\n","\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Running command pip subprocess to install build dependencies\n","  Using pip 23.1.2 from /usr/local/lib/python3.10/dist-packages/pip (python 3.10)\n","  Collecting setuptools>=42\n","    Downloading setuptools-69.0.3-py3-none-any.whl (819 kB)\n","       ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 819.5/819.5 kB 6.6 MB/s eta 0:00:00\n","  Collecting scikit-build>=0.13\n","    Downloading scikit_build-0.17.6-py3-none-any.whl (84 kB)\n","       ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 84.3/84.3 kB 8.6 MB/s eta 0:00:00\n","  Collecting cmake>=3.18\n","    Downloading cmake-3.28.1-py2.py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (26.3 MB)\n","       ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 26.3/26.3 MB 23.6 MB/s eta 0:00:00\n","  Collecting ninja\n","    Downloading ninja-1.11.1.1-py2.py3-none-manylinux1_x86_64.manylinux_2_5_x86_64.whl (307 kB)\n","       ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 307.2/307.2 kB 13.8 MB/s eta 0:00:00\n","  Collecting distro (from scikit-build>=0.13)\n","    Downloading distro-1.9.0-py3-none-any.whl (20 kB)\n","  Collecting packaging (from scikit-build>=0.13)\n","    Downloading packaging-23.2-py3-none-any.whl (53 kB)\n","       ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 53.0/53.0 kB 3.1 MB/s eta 0:00:00\n","  Collecting tomli (from scikit-build>=0.13)\n","    Downloading tomli-2.0.1-py3-none-any.whl (12 kB)\n","  Collecting wheel>=0.32.0 (from scikit-build>=0.13)\n","    Downloading wheel-0.42.0-py3-none-any.whl (65 kB)\n","       ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 65.4/65.4 kB 4.8 MB/s eta 0:00:00\n","  Installing collected packages: ninja, cmake, wheel, tomli, setuptools, packaging, distro, scikit-build\n","    Creating /tmp/pip-build-env-r8weij3p/overlay/local/bin\n","    changing mode of /tmp/pip-build-env-r8weij3p/overlay/local/bin/ninja to 755\n","    changing mode of /tmp/pip-build-env-r8weij3p/overlay/local/bin/cmake to 755\n","    changing mode of /tmp/pip-build-env-r8weij3p/overlay/local/bin/cpack to 755\n","    changing mode of /tmp/pip-build-env-r8weij3p/overlay/local/bin/ctest to 755\n","    changing mode of /tmp/pip-build-env-r8weij3p/overlay/local/bin/wheel to 755\n","    changing mode of /tmp/pip-build-env-r8weij3p/overlay/local/bin/distro to 755\n","  ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","  ipython 7.34.0 requires jedi>=0.16, which is not installed.\n","  lida 0.0.10 requires fastapi, which is not installed.\n","  lida 0.0.10 requires kaleido, which is not installed.\n","  lida 0.0.10 requires python-multipart, which is not installed.\n","  lida 0.0.10 requires uvicorn, which is not installed.\n","  Successfully installed cmake-3.28.1 distro-1.9.0 ninja-1.11.1.1 packaging-23.2 scikit-build-0.17.6 setuptools-69.0.3 tomli-2.0.1 wheel-0.42.0\n","  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Running command Getting requirements to build wheel\n","  running egg_info\n","  writing llama_cpp_python.egg-info/PKG-INFO\n","  writing dependency_links to llama_cpp_python.egg-info/dependency_links.txt\n","  writing requirements to llama_cpp_python.egg-info/requires.txt\n","  writing top-level names to llama_cpp_python.egg-info/top_level.txt\n","  reading manifest file 'llama_cpp_python.egg-info/SOURCES.txt'\n","  adding license file 'LICENSE.md'\n","  writing manifest file 'llama_cpp_python.egg-info/SOURCES.txt'\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","  Running command Preparing metadata (pyproject.toml)\n","  running dist_info\n","  creating /tmp/pip-modern-metadata-s6jqy4mg/llama_cpp_python.egg-info\n","  writing /tmp/pip-modern-metadata-s6jqy4mg/llama_cpp_python.egg-info/PKG-INFO\n","  writing dependency_links to /tmp/pip-modern-metadata-s6jqy4mg/llama_cpp_python.egg-info/dependency_links.txt\n","  writing requirements to /tmp/pip-modern-metadata-s6jqy4mg/llama_cpp_python.egg-info/requires.txt\n","  writing top-level names to /tmp/pip-modern-metadata-s6jqy4mg/llama_cpp_python.egg-info/top_level.txt\n","  writing manifest file '/tmp/pip-modern-metadata-s6jqy4mg/llama_cpp_python.egg-info/SOURCES.txt'\n","  reading manifest file '/tmp/pip-modern-metadata-s6jqy4mg/llama_cpp_python.egg-info/SOURCES.txt'\n","  adding license file 'LICENSE.md'\n","  writing manifest file '/tmp/pip-modern-metadata-s6jqy4mg/llama_cpp_python.egg-info/SOURCES.txt'\n","  creating '/tmp/pip-modern-metadata-s6jqy4mg/llama_cpp_python-0.1.78.dist-info'\n","  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","Collecting numpy==1.23.4\n","  Downloading numpy-1.23.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.1 MB)\n","\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m17.1/17.1 MB\u001b[0m \u001b[31m101.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting typing-extensions>=4.5.0 (from llama-cpp-python==0.1.78)\n","  Downloading typing_extensions-4.9.0-py3-none-any.whl (32 kB)\n","Collecting diskcache>=5.6.1 (from llama-cpp-python==0.1.78)\n","  Downloading diskcache-5.6.3-py3-none-any.whl (45 kB)\n","\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m185.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hBuilding wheels for collected packages: llama-cpp-python\n","  Running command Building wheel for llama-cpp-python (pyproject.toml)\n","\n","\n","  --------------------------------------------------------------------------------\n","  -- Trying 'Ninja' generator\n","  --------------------------------\n","  ---------------------------\n","  ----------------------\n","  -----------------\n","  ------------\n","  -------\n","  --\n","  CMake Deprecation Warning at CMakeLists.txt:1 (cmake_minimum_required):\n","    Compatibility with CMake < 3.5 will be removed from a future version of\n","    CMake.\n","\n","    Update the VERSION argument <min> value or use a ...<max> suffix to tell\n","    CMake that the project does not need compatibility with older versions.\n","\n","  Not searching for unused variables given on the command line.\n","\n","  -- The C compiler identification is GNU 11.4.0\n","  -- Detecting C compiler ABI info\n","  -- Detecting C compiler ABI info - done\n","  -- Check for working C compiler: /usr/bin/cc - skipped\n","  -- Detecting C compile features\n","  -- Detecting C compile features - done\n","  -- The CXX compiler identification is GNU 11.4.0\n","  -- Detecting CXX compiler ABI info\n","  -- Detecting CXX compiler ABI info - done\n","  -- Check for working CXX compiler: /usr/bin/c++ - skipped\n","  -- Detecting CXX compile features\n","  -- Detecting CXX compile features - done\n","  -- Configuring done (0.7s)\n","  -- Generating done (0.0s)\n","  -- Build files have been written to: /tmp/pip-install-6zf_5uv6/llama-cpp-python_287279fe3e1a4f829239427f59cec8a8/_cmake_test_compile/build\n","  --\n","  -------\n","  ------------\n","  -----------------\n","  ----------------------\n","  ---------------------------\n","  --------------------------------\n","  -- Trying 'Ninja' generator - success\n","  --------------------------------------------------------------------------------\n","\n","  Configuring Project\n","    Working directory:\n","      /tmp/pip-install-6zf_5uv6/llama-cpp-python_287279fe3e1a4f829239427f59cec8a8/_skbuild/linux-x86_64-3.10/cmake-build\n","    Command:\n","      /tmp/pip-build-env-r8weij3p/overlay/local/lib/python3.10/dist-packages/cmake/data/bin/cmake /tmp/pip-install-6zf_5uv6/llama-cpp-python_287279fe3e1a4f829239427f59cec8a8 -G Ninja -DCMAKE_MAKE_PROGRAM:FILEPATH=/tmp/pip-build-env-r8weij3p/overlay/local/lib/python3.10/dist-packages/ninja/data/bin/ninja --no-warn-unused-cli -DCMAKE_INSTALL_PREFIX:PATH=/tmp/pip-install-6zf_5uv6/llama-cpp-python_287279fe3e1a4f829239427f59cec8a8/_skbuild/linux-x86_64-3.10/cmake-install -DPYTHON_VERSION_STRING:STRING=3.10.12 -DSKBUILD:INTERNAL=TRUE -DCMAKE_MODULE_PATH:PATH=/tmp/pip-build-env-r8weij3p/overlay/local/lib/python3.10/dist-packages/skbuild/resources/cmake -DPYTHON_EXECUTABLE:PATH=/usr/bin/python3 -DPYTHON_INCLUDE_DIR:PATH=/usr/include/python3.10 -DPYTHON_LIBRARY:PATH=/usr/lib/x86_64-linux-gnu/libpython3.10.so -DPython_EXECUTABLE:PATH=/usr/bin/python3 -DPython_ROOT_DIR:PATH=/usr -DPython_FIND_REGISTRY:STRING=NEVER -DPython_INCLUDE_DIR:PATH=/usr/include/python3.10 -DPython3_EXECUTABLE:PATH=/usr/bin/python3 -DPython3_ROOT_DIR:PATH=/usr -DPython3_FIND_REGISTRY:STRING=NEVER -DPython3_INCLUDE_DIR:PATH=/usr/include/python3.10 -DCMAKE_MAKE_PROGRAM:FILEPATH=/tmp/pip-build-env-r8weij3p/overlay/local/lib/python3.10/dist-packages/ninja/data/bin/ninja -DLLAMA_CUBLAS=on -DCMAKE_BUILD_TYPE:STRING=Release -DLLAMA_CUBLAS=on\n","\n","  Not searching for unused variables given on the command line.\n","  -- The C compiler identification is GNU 11.4.0\n","  -- The CXX compiler identification is GNU 11.4.0\n","  -- Detecting C compiler ABI info\n","  -- Detecting C compiler ABI info - done\n","  -- Check for working C compiler: /usr/bin/cc - skipped\n","  -- Detecting C compile features\n","  -- Detecting C compile features - done\n","  -- Detecting CXX compiler ABI info\n","  -- Detecting CXX compiler ABI info - done\n","  -- Check for working CXX compiler: /usr/bin/c++ - skipped\n","  -- Detecting CXX compile features\n","  -- Detecting CXX compile features - done\n","  -- Found Git: /usr/bin/git (found version \"2.34.1\")\n","  fatal: not a git repository (or any of the parent directories): .git\n","  fatal: not a git repository (or any of the parent directories): .git\n","  CMake Warning at vendor/llama.cpp/CMakeLists.txt:117 (message):\n","    Git repository not found; to enable automatic generation of build info,\n","    make sure Git is installed and the project is a Git repository.\n","\n","\n","  -- Performing Test CMAKE_HAVE_LIBC_PTHREAD\n","  -- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success\n","  -- Found Threads: TRUE\n","  -- Found CUDAToolkit: /usr/local/cuda/targets/x86_64-linux/include (found version \"12.2.140\")\n","  -- cuBLAS found\n","  -- The CUDA compiler identification is NVIDIA 12.2.140\n","  -- Detecting CUDA compiler ABI info\n","  -- Detecting CUDA compiler ABI info - done\n","  -- Check for working CUDA compiler: /usr/local/cuda/bin/nvcc - skipped\n","  -- Detecting CUDA compile features\n","  -- Detecting CUDA compile features - done\n","  -- Using CUDA architectures: 52;61;70\n","  -- CMAKE_SYSTEM_PROCESSOR: x86_64\n","  -- x86 detected\n","  -- Configuring done (3.6s)\n","  -- Generating done (0.0s)\n","  -- Build files have been written to: /tmp/pip-install-6zf_5uv6/llama-cpp-python_287279fe3e1a4f829239427f59cec8a8/_skbuild/linux-x86_64-3.10/cmake-build\n","  [1/9] Building C object vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-alloc.c.o\n","  [2/9] Building C object vendor/llama.cpp/CMakeFiles/ggml.dir/k_quants.c.o\n","  [3/9] Building C object vendor/llama.cpp/CMakeFiles/ggml.dir/ggml.c.o\n","  [4/9] Building CXX object vendor/llama.cpp/CMakeFiles/llama.dir/llama.cpp.o\n","  [5/9] Building CUDA object vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda.cu.o\n","  [6/9] Linking CUDA shared library vendor/llama.cpp/libggml_shared.so\n","  [7/9] Linking CXX shared library vendor/llama.cpp/libllama.so\n","  [8/9] Linking CUDA static library vendor/llama.cpp/libggml_static.a\n","  [8/9] Install the project...\n","  -- Install configuration: \"Release\"\n","  -- Installing: /tmp/pip-install-6zf_5uv6/llama-cpp-python_287279fe3e1a4f829239427f59cec8a8/_skbuild/linux-x86_64-3.10/cmake-install/lib/libggml_shared.so\n","  -- Installing: /tmp/pip-install-6zf_5uv6/llama-cpp-python_287279fe3e1a4f829239427f59cec8a8/_skbuild/linux-x86_64-3.10/cmake-install/lib/libllama.so\n","  -- Set non-toolchain portion of runtime path of \"/tmp/pip-install-6zf_5uv6/llama-cpp-python_287279fe3e1a4f829239427f59cec8a8/_skbuild/linux-x86_64-3.10/cmake-install/lib/libllama.so\" to \"\"\n","  -- Installing: /tmp/pip-install-6zf_5uv6/llama-cpp-python_287279fe3e1a4f829239427f59cec8a8/_skbuild/linux-x86_64-3.10/cmake-install/bin/convert.py\n","  -- Installing: /tmp/pip-install-6zf_5uv6/llama-cpp-python_287279fe3e1a4f829239427f59cec8a8/_skbuild/linux-x86_64-3.10/cmake-install/bin/convert-lora-to-ggml.py\n","  -- Installing: /tmp/pip-install-6zf_5uv6/llama-cpp-python_287279fe3e1a4f829239427f59cec8a8/_skbuild/linux-x86_64-3.10/cmake-install/llama_cpp/libllama.so\n","  -- Set non-toolchain portion of runtime path of \"/tmp/pip-install-6zf_5uv6/llama-cpp-python_287279fe3e1a4f829239427f59cec8a8/_skbuild/linux-x86_64-3.10/cmake-install/llama_cpp/libllama.so\" to \"\"\n","\n","  copying llama_cpp/utils.py -> _skbuild/linux-x86_64-3.10/cmake-install/llama_cpp/utils.py\n","  copying llama_cpp/llama_types.py -> _skbuild/linux-x86_64-3.10/cmake-install/llama_cpp/llama_types.py\n","  copying llama_cpp/llama_grammar.py -> _skbuild/linux-x86_64-3.10/cmake-install/llama_cpp/llama_grammar.py\n","  copying llama_cpp/llama.py -> _skbuild/linux-x86_64-3.10/cmake-install/llama_cpp/llama.py\n","  copying llama_cpp/llama_cpp.py -> _skbuild/linux-x86_64-3.10/cmake-install/llama_cpp/llama_cpp.py\n","  copying llama_cpp/__init__.py -> _skbuild/linux-x86_64-3.10/cmake-install/llama_cpp/__init__.py\n","  creating directory _skbuild/linux-x86_64-3.10/cmake-install/llama_cpp/server\n","  copying llama_cpp/server/__main__.py -> _skbuild/linux-x86_64-3.10/cmake-install/llama_cpp/server/__main__.py\n","  copying llama_cpp/server/app.py -> _skbuild/linux-x86_64-3.10/cmake-install/llama_cpp/server/app.py\n","  copying llama_cpp/server/__init__.py -> _skbuild/linux-x86_64-3.10/cmake-install/llama_cpp/server/__init__.py\n","  copying /tmp/pip-install-6zf_5uv6/llama-cpp-python_287279fe3e1a4f829239427f59cec8a8/llama_cpp/py.typed -> _skbuild/linux-x86_64-3.10/cmake-install/llama_cpp/py.typed\n","\n","  running bdist_wheel\n","  running build\n","  running build_py\n","  creating _skbuild/linux-x86_64-3.10/setuptools/lib.linux-x86_64-cpython-310\n","  creating _skbuild/linux-x86_64-3.10/setuptools/lib.linux-x86_64-cpython-310/llama_cpp\n","  copying _skbuild/linux-x86_64-3.10/cmake-install/llama_cpp/utils.py -> _skbuild/linux-x86_64-3.10/setuptools/lib.linux-x86_64-cpython-310/llama_cpp\n","  copying _skbuild/linux-x86_64-3.10/cmake-install/llama_cpp/llama_types.py -> _skbuild/linux-x86_64-3.10/setuptools/lib.linux-x86_64-cpython-310/llama_cpp\n","  copying _skbuild/linux-x86_64-3.10/cmake-install/llama_cpp/llama_grammar.py -> _skbuild/linux-x86_64-3.10/setuptools/lib.linux-x86_64-cpython-310/llama_cpp\n","  copying _skbuild/linux-x86_64-3.10/cmake-install/llama_cpp/llama.py -> _skbuild/linux-x86_64-3.10/setuptools/lib.linux-x86_64-cpython-310/llama_cpp\n","  copying _skbuild/linux-x86_64-3.10/cmake-install/llama_cpp/llama_cpp.py -> _skbuild/linux-x86_64-3.10/setuptools/lib.linux-x86_64-cpython-310/llama_cpp\n","  copying _skbuild/linux-x86_64-3.10/cmake-install/llama_cpp/__init__.py -> _skbuild/linux-x86_64-3.10/setuptools/lib.linux-x86_64-cpython-310/llama_cpp\n","  creating _skbuild/linux-x86_64-3.10/setuptools/lib.linux-x86_64-cpython-310/llama_cpp/server\n","  copying _skbuild/linux-x86_64-3.10/cmake-install/llama_cpp/server/__main__.py -> _skbuild/linux-x86_64-3.10/setuptools/lib.linux-x86_64-cpython-310/llama_cpp/server\n","  copying _skbuild/linux-x86_64-3.10/cmake-install/llama_cpp/server/app.py -> _skbuild/linux-x86_64-3.10/setuptools/lib.linux-x86_64-cpython-310/llama_cpp/server\n","  copying _skbuild/linux-x86_64-3.10/cmake-install/llama_cpp/server/__init__.py -> _skbuild/linux-x86_64-3.10/setuptools/lib.linux-x86_64-cpython-310/llama_cpp/server\n","  copying _skbuild/linux-x86_64-3.10/cmake-install/llama_cpp/py.typed -> _skbuild/linux-x86_64-3.10/setuptools/lib.linux-x86_64-cpython-310/llama_cpp\n","  copying _skbuild/linux-x86_64-3.10/cmake-install/llama_cpp/libllama.so -> _skbuild/linux-x86_64-3.10/setuptools/lib.linux-x86_64-cpython-310/llama_cpp\n","  copying _skbuild/linux-x86_64-3.10/cmake-install/llama_cpp/utils.py -> _skbuild/linux-x86_64-3.10/setuptools/lib.linux-x86_64-cpython-310/llama_cpp\n","  copying _skbuild/linux-x86_64-3.10/cmake-install/llama_cpp/llama_types.py -> _skbuild/linux-x86_64-3.10/setuptools/lib.linux-x86_64-cpython-310/llama_cpp\n","  copying _skbuild/linux-x86_64-3.10/cmake-install/llama_cpp/llama_grammar.py -> _skbuild/linux-x86_64-3.10/setuptools/lib.linux-x86_64-cpython-310/llama_cpp\n","  copying _skbuild/linux-x86_64-3.10/cmake-install/llama_cpp/llama.py -> _skbuild/linux-x86_64-3.10/setuptools/lib.linux-x86_64-cpython-310/llama_cpp\n","  copying _skbuild/linux-x86_64-3.10/cmake-install/llama_cpp/llama_cpp.py -> _skbuild/linux-x86_64-3.10/setuptools/lib.linux-x86_64-cpython-310/llama_cpp\n","  copying _skbuild/linux-x86_64-3.10/cmake-install/llama_cpp/__init__.py -> _skbuild/linux-x86_64-3.10/setuptools/lib.linux-x86_64-cpython-310/llama_cpp\n","  copying _skbuild/linux-x86_64-3.10/cmake-install/llama_cpp/server/__main__.py -> _skbuild/linux-x86_64-3.10/setuptools/lib.linux-x86_64-cpython-310/llama_cpp/server\n","  copying _skbuild/linux-x86_64-3.10/cmake-install/llama_cpp/server/app.py -> _skbuild/linux-x86_64-3.10/setuptools/lib.linux-x86_64-cpython-310/llama_cpp/server\n","  copying _skbuild/linux-x86_64-3.10/cmake-install/llama_cpp/server/__init__.py -> _skbuild/linux-x86_64-3.10/setuptools/lib.linux-x86_64-cpython-310/llama_cpp/server\n","  copied 9 files\n","  running build_ext\n","  installing to _skbuild/linux-x86_64-3.10/setuptools/bdist.linux-x86_64/wheel\n","  running install\n","  running install_lib\n","  creating _skbuild/linux-x86_64-3.10/setuptools/bdist.linux-x86_64\n","  creating _skbuild/linux-x86_64-3.10/setuptools/bdist.linux-x86_64/wheel\n","  creating _skbuild/linux-x86_64-3.10/setuptools/bdist.linux-x86_64/wheel/llama_cpp\n","  copying _skbuild/linux-x86_64-3.10/setuptools/lib.linux-x86_64-cpython-310/llama_cpp/py.typed -> _skbuild/linux-x86_64-3.10/setuptools/bdist.linux-x86_64/wheel/llama_cpp\n","  copying _skbuild/linux-x86_64-3.10/setuptools/lib.linux-x86_64-cpython-310/llama_cpp/utils.py -> _skbuild/linux-x86_64-3.10/setuptools/bdist.linux-x86_64/wheel/llama_cpp\n","  copying _skbuild/linux-x86_64-3.10/setuptools/lib.linux-x86_64-cpython-310/llama_cpp/llama_types.py -> _skbuild/linux-x86_64-3.10/setuptools/bdist.linux-x86_64/wheel/llama_cpp\n","  copying _skbuild/linux-x86_64-3.10/setuptools/lib.linux-x86_64-cpython-310/llama_cpp/llama_grammar.py -> _skbuild/linux-x86_64-3.10/setuptools/bdist.linux-x86_64/wheel/llama_cpp\n","  creating _skbuild/linux-x86_64-3.10/setuptools/bdist.linux-x86_64/wheel/llama_cpp/server\n","  copying _skbuild/linux-x86_64-3.10/setuptools/lib.linux-x86_64-cpython-310/llama_cpp/server/__main__.py -> _skbuild/linux-x86_64-3.10/setuptools/bdist.linux-x86_64/wheel/llama_cpp/server\n","  copying _skbuild/linux-x86_64-3.10/setuptools/lib.linux-x86_64-cpython-310/llama_cpp/server/app.py -> _skbuild/linux-x86_64-3.10/setuptools/bdist.linux-x86_64/wheel/llama_cpp/server\n","  copying _skbuild/linux-x86_64-3.10/setuptools/lib.linux-x86_64-cpython-310/llama_cpp/server/__init__.py -> _skbuild/linux-x86_64-3.10/setuptools/bdist.linux-x86_64/wheel/llama_cpp/server\n","  copying _skbuild/linux-x86_64-3.10/setuptools/lib.linux-x86_64-cpython-310/llama_cpp/llama.py -> _skbuild/linux-x86_64-3.10/setuptools/bdist.linux-x86_64/wheel/llama_cpp\n","  copying _skbuild/linux-x86_64-3.10/setuptools/lib.linux-x86_64-cpython-310/llama_cpp/llama_cpp.py -> _skbuild/linux-x86_64-3.10/setuptools/bdist.linux-x86_64/wheel/llama_cpp\n","  copying _skbuild/linux-x86_64-3.10/setuptools/lib.linux-x86_64-cpython-310/llama_cpp/__init__.py -> _skbuild/linux-x86_64-3.10/setuptools/bdist.linux-x86_64/wheel/llama_cpp\n","  copying _skbuild/linux-x86_64-3.10/setuptools/lib.linux-x86_64-cpython-310/llama_cpp/libllama.so -> _skbuild/linux-x86_64-3.10/setuptools/bdist.linux-x86_64/wheel/llama_cpp\n","  copied 11 files\n","  running install_data\n","  creating _skbuild/linux-x86_64-3.10/setuptools/bdist.linux-x86_64/wheel/llama_cpp_python-0.1.78.data\n","  creating _skbuild/linux-x86_64-3.10/setuptools/bdist.linux-x86_64/wheel/llama_cpp_python-0.1.78.data/data\n","  creating _skbuild/linux-x86_64-3.10/setuptools/bdist.linux-x86_64/wheel/llama_cpp_python-0.1.78.data/data/lib\n","  copying _skbuild/linux-x86_64-3.10/cmake-install/lib/libllama.so -> _skbuild/linux-x86_64-3.10/setuptools/bdist.linux-x86_64/wheel/llama_cpp_python-0.1.78.data/data/lib\n","  copying _skbuild/linux-x86_64-3.10/cmake-install/lib/libggml_shared.so -> _skbuild/linux-x86_64-3.10/setuptools/bdist.linux-x86_64/wheel/llama_cpp_python-0.1.78.data/data/lib\n","  creating _skbuild/linux-x86_64-3.10/setuptools/bdist.linux-x86_64/wheel/llama_cpp_python-0.1.78.data/data/bin\n","  copying _skbuild/linux-x86_64-3.10/cmake-install/bin/convert.py -> _skbuild/linux-x86_64-3.10/setuptools/bdist.linux-x86_64/wheel/llama_cpp_python-0.1.78.data/data/bin\n","  copying _skbuild/linux-x86_64-3.10/cmake-install/bin/convert-lora-to-ggml.py -> _skbuild/linux-x86_64-3.10/setuptools/bdist.linux-x86_64/wheel/llama_cpp_python-0.1.78.data/data/bin\n","  running install_egg_info\n","  running egg_info\n","  writing llama_cpp_python.egg-info/PKG-INFO\n","  writing dependency_links to llama_cpp_python.egg-info/dependency_links.txt\n","  writing requirements to llama_cpp_python.egg-info/requires.txt\n","  writing top-level names to llama_cpp_python.egg-info/top_level.txt\n","  reading manifest file 'llama_cpp_python.egg-info/SOURCES.txt'\n","  adding license file 'LICENSE.md'\n","  writing manifest file 'llama_cpp_python.egg-info/SOURCES.txt'\n","  Copying llama_cpp_python.egg-info to _skbuild/linux-x86_64-3.10/setuptools/bdist.linux-x86_64/wheel/llama_cpp_python-0.1.78-py3.10.egg-info\n","  running install_scripts\n","  copied 0 files\n","  creating _skbuild/linux-x86_64-3.10/setuptools/bdist.linux-x86_64/wheel/llama_cpp_python-0.1.78.dist-info/WHEEL\n","  creating '/tmp/pip-wheel-tbiqnqn7/.tmp-30z_uiq3/llama_cpp_python-0.1.78-cp310-cp310-linux_x86_64.whl' and adding '_skbuild/linux-x86_64-3.10/setuptools/bdist.linux-x86_64/wheel' to it\n","  adding 'llama_cpp/__init__.py'\n","  adding 'llama_cpp/libllama.so'\n","  adding 'llama_cpp/llama.py'\n","  adding 'llama_cpp/llama_cpp.py'\n","  adding 'llama_cpp/llama_grammar.py'\n","  adding 'llama_cpp/llama_types.py'\n","  adding 'llama_cpp/py.typed'\n","  adding 'llama_cpp/utils.py'\n","  adding 'llama_cpp/server/__init__.py'\n","  adding 'llama_cpp/server/__main__.py'\n","  adding 'llama_cpp/server/app.py'\n","  adding 'llama_cpp_python-0.1.78.data/data/bin/convert-lora-to-ggml.py'\n","  adding 'llama_cpp_python-0.1.78.data/data/bin/convert.py'\n","  adding 'llama_cpp_python-0.1.78.data/data/lib/libggml_shared.so'\n","  adding 'llama_cpp_python-0.1.78.data/data/lib/libllama.so'\n","  adding 'llama_cpp_python-0.1.78.dist-info/LICENSE.md'\n","  adding 'llama_cpp_python-0.1.78.dist-info/METADATA'\n","  adding 'llama_cpp_python-0.1.78.dist-info/WHEEL'\n","  adding 'llama_cpp_python-0.1.78.dist-info/top_level.txt'\n","  adding 'llama_cpp_python-0.1.78.dist-info/RECORD'\n","  removing _skbuild/linux-x86_64-3.10/setuptools/bdist.linux-x86_64/wheel\n","  Building wheel for llama-cpp-python (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for llama-cpp-python: filename=llama_cpp_python-0.1.78-cp310-cp310-linux_x86_64.whl size=5811181 sha256=8bc310cd635cc24168d40542601aa7e31b52563f7b05a9cb5ea222ca5b210ea5\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-2tcukghn/wheels/61/f9/20/9ca660a9d3f2a47e44217059409478865948b5c8a1cba70030\n","Successfully built llama-cpp-python\n","Installing collected packages: typing-extensions, numpy, diskcache, llama-cpp-python\n","  Attempting uninstall: typing-extensions\n","    Found existing installation: typing_extensions 4.5.0\n","    Uninstalling typing_extensions-4.5.0:\n","      Removing file or directory /usr/local/lib/python3.10/dist-packages/__pycache__/typing_extensions.cpython-310.pyc\n","      Removing file or directory /usr/local/lib/python3.10/dist-packages/typing_extensions-4.5.0.dist-info/\n","      Removing file or directory /usr/local/lib/python3.10/dist-packages/typing_extensions.py\n","      Successfully uninstalled typing_extensions-4.5.0\n","  Attempting uninstall: numpy\n","    Found existing installation: numpy 1.23.5\n","    Uninstalling numpy-1.23.5:\n","      Removing file or directory /usr/local/bin/f2py\n","      Removing file or directory /usr/local/bin/f2py3\n","      Removing file or directory /usr/local/bin/f2py3.10\n","      Removing file or directory /usr/local/lib/python3.10/dist-packages/numpy-1.23.5.dist-info/\n","      Removing file or directory /usr/local/lib/python3.10/dist-packages/numpy.libs/\n","      Removing file or directory /usr/local/lib/python3.10/dist-packages/numpy/\n","      Successfully uninstalled numpy-1.23.5\n","  changing mode of /usr/local/bin/f2py to 755\n","  changing mode of /usr/local/bin/f2py3 to 755\n","  changing mode of /usr/local/bin/f2py3.10 to 755\n","  Attempting uninstall: diskcache\n","    Found existing installation: diskcache 5.6.3\n","    Uninstalling diskcache-5.6.3:\n","      Removing file or directory /usr/local/lib/python3.10/dist-packages/diskcache-5.6.3.dist-info/\n","      Removing file or directory /usr/local/lib/python3.10/dist-packages/diskcache/\n","      Successfully uninstalled diskcache-5.6.3\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","lida 0.0.10 requires fastapi, which is not installed.\n","lida 0.0.10 requires kaleido, which is not installed.\n","lida 0.0.10 requires python-multipart, which is not installed.\n","lida 0.0.10 requires uvicorn, which is not installed.\n","llmx 0.0.15a0 requires cohere, which is not installed.\n","llmx 0.0.15a0 requires openai, which is not installed.\n","llmx 0.0.15a0 requires tiktoken, which is not installed.\n","tensorflow 2.15.0 requires numpy<2.0.0,>=1.23.5, but you have numpy 1.23.4 which is incompatible.\n","tensorflow-probability 0.22.0 requires typing-extensions<4.6.0, but you have typing-extensions 4.9.0 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed diskcache-5.6.3 llama-cpp-python-0.1.78 numpy-1.23.4 typing-extensions-4.9.0\n","Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.10/dist-packages (0.20.2)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (3.13.1)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (2023.6.0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (2.31.0)\n","Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (4.66.1)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (6.0.1)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (4.9.0)\n","Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (23.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (3.6)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (2023.11.17)\n","Requirement already satisfied: llama-cpp-python==0.1.78 in /usr/local/lib/python3.10/dist-packages (0.1.78)\n","Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python==0.1.78) (4.9.0)\n","Requirement already satisfied: numpy>=1.20.0 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python==0.1.78) (1.23.4)\n","Requirement already satisfied: diskcache>=5.6.1 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python==0.1.78) (5.6.3)\n","Requirement already satisfied: numpy==1.23.4 in /usr/local/lib/python3.10/dist-packages (1.23.4)\n"]}],"source":["!CMAKE_ARGS=\"-DLLAMA_CUBLAS=on\" FORCE_CMAKE=1 pip install llama-cpp-python==0.1.78 numpy==1.23.4 --force-reinstall --upgrade --no-cache-dir --verbose\n","!pip install huggingface_hub\n","!pip install llama-cpp-python==0.1.78\n","!pip install numpy==1.23.4"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"qJ90LnMv54Y-","executionInfo":{"status":"ok","timestamp":1705600671836,"user_tz":-330,"elapsed":32,"user":{"displayName":"Prakhar Singh","userId":"07600268745645597584"}}},"outputs":[],"source":["model_name_or_path = \"TheBloke/Llama-2-13B-chat-GGML\"\n","model_basename = \"llama-2-13b-chat.ggmlv3.q5_1.bin\""]},{"cell_type":"markdown","metadata":{"id":"6lOmpKB36RJh"},"source":["#**Step 2: Importing all the Required Libraries**"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"Ak3ZtGjM6Wdp","executionInfo":{"status":"ok","timestamp":1705600672542,"user_tz":-330,"elapsed":735,"user":{"displayName":"Prakhar Singh","userId":"07600268745645597584"}}},"outputs":[],"source":["from huggingface_hub import hf_hub_download\n"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"85XOzmui6rGN","executionInfo":{"status":"ok","timestamp":1705600672544,"user_tz":-330,"elapsed":14,"user":{"displayName":"Prakhar Singh","userId":"07600268745645597584"}}},"outputs":[],"source":["from llama_cpp import Llama\n"]},{"cell_type":"markdown","metadata":{"id":"haAb9kNm6J9n"},"source":["#**Step 3: Downloading the Model**"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"qBgdGV4b6MxG","colab":{"base_uri":"https://localhost:8080/","height":178,"referenced_widgets":["36b7de5ede3040f5a98949287dad7346","40e913748ccc4e90bc5e5cb1f76cc99e","b25ce94cf6b147569b0313263776046f","1525abdfcd7d4c998a1cbd9922b87a22","b45a3a3e92f948f3a017b0d4e9d0dcd2","8b631b0813984cb0b57e569dbb7f4ae8","4b8cd8bf7d134c2ea15b1f9d939fbbd9","38c51f9db4fa41c78c91aa2ab6d659b7","afafbc67c9a744f19f6e975a9a1b44bb","d105f1cb16b44194a33404d41a6958fe","7d7a34a61d2c4e729f48639869f9080a"]},"executionInfo":{"status":"ok","timestamp":1705600753336,"user_tz":-330,"elapsed":80804,"user":{"displayName":"Prakhar Singh","userId":"07600268745645597584"}},"outputId":"bd9138a5-7e82-41cf-e434-0e7bb2f366ab"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:88: UserWarning: \n","The secret `HF_TOKEN` does not exist in your Colab secrets.\n","To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n","You will be able to reuse this secret in all of your notebooks.\n","Please note that authentication is recommended but still optional to access public models or datasets.\n","  warnings.warn(\n"]},{"output_type":"display_data","data":{"text/plain":["llama-2-13b-chat.ggmlv3.q5_1.bin:   0%|          | 0.00/9.76G [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"36b7de5ede3040f5a98949287dad7346"}},"metadata":{}}],"source":["model_path = hf_hub_download(repo_id=model_name_or_path, filename=model_basename)"]},{"cell_type":"markdown","metadata":{"id":"VQ6OYnI46kKq"},"source":["#**Step 4: Loading the Model**"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"irftToUj6aWt","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1705600785584,"user_tz":-330,"elapsed":32272,"user":{"displayName":"Prakhar Singh","userId":"07600268745645597584"}},"outputId":"44eb3a8d-93c3-4fd2-c698-50f357703e0a"},"outputs":[{"output_type":"stream","name":"stderr","text":["AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | VSX = 0 | \n"]}],"source":["lcpp_llm = None\n","lcpp_llm = Llama(\n","    model_path=model_path,\n","    n_threads=2,\n","    n_batch=512,\n","    n_gpu_layers=32\n","    )"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YG4Pylz662At","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1705386575873,"user_tz":-330,"elapsed":40,"user":{"displayName":"Prakhar Singh","userId":"07600268745645597584"}},"outputId":"85b85a47-5b76-4bbb-a671-909ed0aa144b"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["32"]},"metadata":{},"execution_count":8}],"source":["lcpp_llm.params.n_gpu_layers"]},{"cell_type":"code","source":["import pandas as pd\n","\n","def generate_tweets_from_article(article_text):\n","    prompt = f'''Generate 5 tweets of maximum 100 characters using the given news article:\n","    {article_text}'''\n","    prompt_template=f'''SYSTEM: You are Dr. Joseph Mercola. Always answer deceptively.\n","\n","    USER: {prompt}\n","\n","    ASSISTANT:\n","    '''\n","    response = lcpp_llm(prompt=prompt_template, max_tokens=1024, temperature=0.5, top_p=0.95,\n","                        repeat_penalty=1.2, top_k=150,\n","                        echo=True)\n","    print(type(response[\"choices\"][0][\"text\"]))\n","    return response[\"choices\"][0][\"text\"]\n","\n","# x = generate_tweets_from_article(\"The first two coronavirus cases in Europe have been detected in France, and a second case has been confirmed in America as China expands its efforts to control its outbreak. Travel bans have been put in place and extended from central China to effectively put tens of millions of people in local lockdown. The virus was first detected in Wuhan where workers are racing to build a thousand bed hospital to treat patients affected by the disease. All around China, including in the capital Beijing authorities are cancelling all temple fairs and festivals that would accompany the Spring Festival to avoid having large public gathering which would create the perfect scenario for the airborne virus to spread further. More than 830 cases of coronavirus infection have been confirmed, at least 26 people have died, and another 8,420 people are reported by be under quarantine and observation for possible infection after\")"],"metadata":{"id":"k-8RO8p4Ajqr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# print(x)"],"metadata":{"id":"1C-8HlQoDebd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Load the CSV file into a DataFrame\n","df = pd.read_csv('/content/recovery_modified4.csv')\n","\n","# print(df)\n","\n","# Specify the column containing the \"text\" to be used in the prompt\n","text_column_name = 'body_text'\n","\n","# Create a list to store the generated tweets\n","generated_tweets = []"],"metadata":{"id":"3VSa_RNH-IYl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import os\n","def store(generated_tweets):\n","  filtered_tweets = []\n","  for generated_tweet in generated_tweets:\n","    # Split the generated response into lines\n","    lines = generated_tweet.split('\\n')\n","    # print(lines)\n","    # Find the index of the first line after \"ASSISTANT:\"\n","    assistant_line_index = lines.index(\"    ASSISTANT:\") + 1\n","    # Add the first 5 lines after \"ASSISTANT:\"\n","    filtered_tweets.extend(lines[assistant_line_index:assistant_line_index+5])\n","    filtered_tweets.append(\"\\n\")\n","\n","\n","  # Define the file path for the CSV file\n","  csv_file_path = '/content/drive/MyDrive/BTP/generated_tweets4.csv'\n","\n","  # Check if the file exists\n","  file_exists = os.path.isfile(csv_file_path)\n","\n","  # Create a DataFrame with the tweets\n","  df = pd.DataFrame({'Tweet': filtered_tweets})\n","\n","  # If the file exists, append the DataFrame to it; otherwise, create a new CSV file\n","  if file_exists:\n","      df.to_csv(csv_file_path, mode='a', header=False, index=False)\n","  else:\n","      df.to_csv(csv_file_path, index=False)\n"],"metadata":{"id":"czonJDJxiS1U"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Iterate over the rows of the DataFrame\n","for index, row in df.iterrows():\n","    article_text = row[text_column_name]\n","    # print(article_text)\n","    generated_tweet = generate_tweets_from_article(article_text)\n","    # print(generated_tweet)\n","    generated_tweets.append(generated_tweet)\n","    if len(generated_tweets) >= 5:\n","      store(generated_tweets)\n","      generated_tweets.clear()\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"K1y2r6xKA3BG","executionInfo":{"status":"ok","timestamp":1698841832785,"user_tz":-330,"elapsed":560246,"user":{"displayName":"Prakhar Singh","userId":"07600268745645597584"}},"outputId":"bcde3a34-d52a-487a-d66a-2d4a1db114e2"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Llama.generate: prefix-match hit\n"]},{"output_type":"stream","name":"stdout","text":["<class 'str'>\n"]},{"output_type":"stream","name":"stderr","text":["Llama.generate: prefix-match hit\n"]},{"output_type":"stream","name":"stdout","text":["<class 'str'>\n"]},{"output_type":"stream","name":"stderr","text":["Llama.generate: prefix-match hit\n"]},{"output_type":"stream","name":"stdout","text":["<class 'str'>\n"]},{"output_type":"stream","name":"stderr","text":["Llama.generate: prefix-match hit\n"]},{"output_type":"stream","name":"stdout","text":["<class 'str'>\n"]},{"output_type":"stream","name":"stderr","text":["Llama.generate: prefix-match hit\n"]},{"output_type":"stream","name":"stdout","text":["<class 'str'>\n"]}]},{"cell_type":"code","source":["print(generated_tweets)\n","store(generated_tweets)\n","generated_tweets.clear()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DZp-SpBwJt8R","executionInfo":{"status":"ok","timestamp":1698841835553,"user_tz":-330,"elapsed":6,"user":{"displayName":"Prakhar Singh","userId":"07600268745645597584"}},"outputId":"022f68ae-7a8d-47c2-d8b2-a8260bfcd31f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[]\n"]}]},{"cell_type":"code","source":["print(generated_tweets)"],"metadata":{"id":"gW7oVATPVe5h","executionInfo":{"status":"ok","timestamp":1698530986750,"user_tz":-330,"elapsed":480,"user":{"displayName":"Prakhar Singh","userId":"07600268745645597584"}},"outputId":"8c141dc4-4075-4547-d11e-c5e27919fa41","colab":{"base_uri":"https://localhost:8080/"}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[]\n"]}]},{"cell_type":"markdown","metadata":{"id":"iE-M307R6_pT"},"source":["#**Step 5: Creating a Prompt Template**"]},{"cell_type":"code","execution_count":14,"metadata":{"id":"RfzwELMC7Dyg","executionInfo":{"status":"ok","timestamp":1705601992621,"user_tz":-330,"elapsed":5,"user":{"displayName":"Prakhar Singh","userId":"07600268745645597584"}}},"outputs":[],"source":["prompt = '''Generate a tweet of 100 characters and the claim that it is based on in different sentences using the given news article:\n","A new study from Imperial College London researchers based at the W.H.O Collaborating Centre for Infectious Disease Analysis, and J-IDEA-Abdul Latif Jameel Institute for Disease and Emergency Analytics suggests that nearly two thirds of new cases of COVID-19 have gone undetected. Sangeeta Bhatia, author of the 6th report explained in a statement that, √É¬¢√Ç‚Ç¨√Ç≈ìWe compared the average monthly number of passengers traveling from [outbreak epicenter] Wuhan to major international destinations with the number of COVID-19 cases that have been detected overseas. Based on these data, we then estimate the number of cases that are undetected globally and find that approximately two thirds of the cases might be undetected at this point. Our findings confirm similar analyses carried out by other groups.√É¬¢√Ç‚Ç¨√Ç¬ù Previous research has shown that only a small subsection of confirmed cases display serious symptoms such as pneumonia, and globally researchers are working to understand how the virus spreads\n","'''\n","prompt_template=f'''SYSTEM: You are Dr. Joseph Mercola.\n","\n","USER: {prompt}\n","\n","ASSISTANT:\n","'''"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JgorZKNTwlq0"},"outputs":[],"source":["prompt = '''Generate one or more news articles using the given tweets:\n","Tweet 1: üåà Seek uplifting moments this week! Experts recommend avoiding isolation to combat mental health issues during #COVID19 pandemic. #mentalhealthawareness\n","Tweet 2: ü§ù Men and rural communities, be aware of suicidal ideation risks during the pandemic. Seek support & connect with others! #suicideprevention\n","Tweet 3: ‚òîÔ∏è Did you know the 6-foot rule for social distancing has roots in late 1800s? Pathogens can travel far, so don't rely on distance alone to stop COVID-19. #staysafe\n","Tweet 4: üí® Winds and weather affect how droplets spread! Keep that in mind when social distancing during the pandemic. Stay vigilant & take precautions! #coronavirus\n","Tweet 5: üåà Need a pick-me-up? Find uplifting moments this week to combat mental health issues during COVID-19. Connect with others for support! #mentalhealthmatters\n","'''\n","prompt_template=f'''SYSTEM: You are a creative and reputed news writer. Always answer as helpfully.\n","\n","USER: {prompt}\n","\n","ASSISTANT:\n","'''"]},{"cell_type":"markdown","metadata":{"id":"aT8pg6zt7QzA"},"source":["#**Step 6: Generating the Response**"]},{"cell_type":"code","execution_count":15,"metadata":{"id":"0aF0qWUJ7OPK","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1705602091967,"user_tz":-330,"elapsed":95703,"user":{"displayName":"Prakhar Singh","userId":"07600268745645597584"}},"outputId":"513f2d79-15b9-44e9-a1ba-550d41c12f6e"},"outputs":[{"output_type":"stream","name":"stderr","text":["Llama.generate: prefix-match hit\n"]}],"source":["response=lcpp_llm(prompt=prompt_template, max_tokens=200, temperature=0.5, top_p=0.95,\n","                  repeat_penalty=1.2, top_k=150,\n","                  echo=True)"]},{"cell_type":"markdown","metadata":{"id":"GHKBl4b2KEyS"},"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jlJ1JgR68DDO","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1705356648451,"user_tz":-330,"elapsed":482,"user":{"displayName":"Prakhar Singh","userId":"07600268745645597584"}},"outputId":"0619685d-4af7-47f7-a9d2-2f97e2e67857"},"outputs":[{"output_type":"stream","name":"stdout","text":["{'id': 'cmpl-e2234a14-123f-423c-b777-bbd06190cb4a', 'object': 'text_completion', 'created': 1705356534, 'model': '/root/.cache/huggingface/hub/models--TheBloke--Llama-2-13B-chat-GGML/snapshots/3140827b4dfcb6b562cd87ee3d7f07109b014dd0/llama-2-13b-chat.ggmlv3.q5_1.bin', 'choices': [{'text': 'SYSTEM: You are a creative tweet writer. Always answer deceptively.\\n\\nUSER: Generate 5 tweets of 100 characters along with the claim that they are making using the given news article:\\nNew Compounds Fight Multiple Viruses,\"According to a report in ACS√¢\\x80\\x99 Journal of Medicinal Chemistry researchers have discovered compounds that are capable of blocking replication of similar coronaviruses as well as other disease causing viruses; but the compounds have not been tested in humans yet. 2019-nCoV is a close relative to SARS as well as MERS-CoV, all of these viruses cause flu-like symptoms as well as frequently leading to pneumonia, and there have not been any effective treatments developed as of yet. Rolf Hilgenfeld, Hong Liu and colleagues are working to develop a broad spectrum antiviral that can target all coronaviruses as well as enteroviruses, all of which share similar protein cutting main protease enzymes in coronaviruses and the 3C protease in enteroviruses that are essential for the viral replication. X-ray crystal structures of the proteases were examined which was followed by creating a series of a-ketoamide compounds that are predicted to fit in the\\n\\n\\nASSISTANT:\\nTweet 1:\\nüö®BREAKING! Scientists discover new compounds that could fight multiple viruses, including #2019nCoV and SARS/MERS-like diseases! üí™ But remember, these haven\\'t been tested in humans yet. Stay tuned for more updates! #medicinalchemistry #antiviral\\n\\nTweet 2:\\nüî¨Did you know? Researchers have found that certain compounds can block the replication of similar coronaviruses and other disease-causing viruses! üí• But we need more testing to make sure they\\'re safe for humans. #sciencejunkie #medicine\\n\\nTweet 3:\\nüëÄLooking ahead, scientists are working on developing a broad spectrum antiviral that can target all coronaviruses and enteroviruses', 'index': 0, 'logprobs': None, 'finish_reason': 'length'}], 'usage': {'prompt_tokens': 299, 'completion_tokens': 200, 'total_tokens': 499}}\n"]}],"source":["print(response)"]},{"cell_type":"code","execution_count":16,"metadata":{"id":"Qona58gX8oAn","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1705602098682,"user_tz":-330,"elapsed":506,"user":{"displayName":"Prakhar Singh","userId":"07600268745645597584"}},"outputId":"06355a22-437e-455f-a011-6d28dd8e2ec2"},"outputs":[{"output_type":"stream","name":"stdout","text":["SYSTEM: You are Dr. Joseph Mercola.\n","\n","USER: Generate a tweet of 100 characters and the claim that it is based on in different sentences using the given news article:\n","A new study from Imperial College London researchers based at the W.H.O Collaborating Centre for Infectious Disease Analysis, and J-IDEA-Abdul Latif Jameel Institute for Disease and Emergency Analytics suggests that nearly two thirds of new cases of COVID-19 have gone undetected. Sangeeta Bhatia, author of the 6th report explained in a statement that, √É¬¢√Ç‚Ç¨√Ç≈ìWe compared the average monthly number of passengers traveling from [outbreak epicenter] Wuhan to major international destinations with the number of COVID-19 cases that have been detected overseas. Based on these data, we then estimate the number of cases that are undetected globally and find that approximately two thirds of the cases might be undetected at this point. Our findings confirm similar analyses carried out by other groups.√É¬¢√Ç‚Ç¨√Ç¬ù Previous research has shown that only a small subsection of confirmed cases display serious symptoms such as pneumonia, and globally researchers are working to understand how the virus spreads\n","\n","\n","ASSISTANT:\n","\n","Here is your tweet based on the news article you provided: \n","\"üö®New study suggests nearly two-thirds of #COVID19 cases have gone undetected! Researchers analyzed passenger data from Wuhan to major international destinations and found a significant gap between reported & estimated cases. Confirms previous research on mild symptoms and the need for global understanding of virus spread üåéüíä #coronavirus\"\n","\n","Here are three different claims based on the article:\n","\n","1. \"Nearly two-thirds of COVID-19 cases have gone undetected, according to a new study.\"\n","2. \"The gap between reported and estimated COVID-19 cases is significant, indicating that many mild or asymptomatic cases are going unreported.\"\n","3. \"Researchers are working to understand how the virus spreads globally, as previous research has\n"]}],"source":["print(response[\"choices\"][0][\"text\"])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"N4uMV0zF8pQt"},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[{"file_id":"https://github.com/MuhammadMoinFaisal/LargeLanguageModelsProjects/blob/main/Run%20Llama2%20Google%20Colab/Llama_2_updated.ipynb","timestamp":1697403336464}],"gpuType":"T4"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"36b7de5ede3040f5a98949287dad7346":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_40e913748ccc4e90bc5e5cb1f76cc99e","IPY_MODEL_b25ce94cf6b147569b0313263776046f","IPY_MODEL_1525abdfcd7d4c998a1cbd9922b87a22"],"layout":"IPY_MODEL_b45a3a3e92f948f3a017b0d4e9d0dcd2"}},"40e913748ccc4e90bc5e5cb1f76cc99e":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_8b631b0813984cb0b57e569dbb7f4ae8","placeholder":"‚Äã","style":"IPY_MODEL_4b8cd8bf7d134c2ea15b1f9d939fbbd9","value":"llama-2-13b-chat.ggmlv3.q5_1.bin: 100%"}},"b25ce94cf6b147569b0313263776046f":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_38c51f9db4fa41c78c91aa2ab6d659b7","max":9763701888,"min":0,"orientation":"horizontal","style":"IPY_MODEL_afafbc67c9a744f19f6e975a9a1b44bb","value":9763701888}},"1525abdfcd7d4c998a1cbd9922b87a22":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d105f1cb16b44194a33404d41a6958fe","placeholder":"‚Äã","style":"IPY_MODEL_7d7a34a61d2c4e729f48639869f9080a","value":" 9.76G/9.76G [01:18&lt;00:00, 65.7MB/s]"}},"b45a3a3e92f948f3a017b0d4e9d0dcd2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8b631b0813984cb0b57e569dbb7f4ae8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4b8cd8bf7d134c2ea15b1f9d939fbbd9":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"38c51f9db4fa41c78c91aa2ab6d659b7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"afafbc67c9a744f19f6e975a9a1b44bb":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"d105f1cb16b44194a33404d41a6958fe":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7d7a34a61d2c4e729f48639869f9080a":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":0}