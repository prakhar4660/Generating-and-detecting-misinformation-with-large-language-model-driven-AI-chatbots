{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"https://github.com/krishnaik06/Huggingfacetransformer/blob/main/Custom_Sentiment_Analysis.ipynb","timestamp":1698556434734}],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["import pandas as pd\n","df = pd.read_csv('/content/Constraint_Train.csv', encoding='ISO-8859-1')\n","df = df.head(100)\n","train_texts=list(df['tweet'])\n","train_labels=list(df['label'])\n","train_labels=list(pd.get_dummies(train_labels,drop_first=True)['real'])\n","\n","df = pd.read_csv('/content/Constraint_Val.csv', encoding='ISO-8859-1')\n","df = df.head(30)\n","val_texts=list(df['tweet'])\n","val_labels=list(df['label'])\n","val_labels=list(pd.get_dummies(val_labels,drop_first=True)['real'])\n","\n","df = pd.read_csv('/content/Constraint_test.csv', encoding='ISO-8859-1')\n","df = df.head(30)\n","test_texts=list(df['tweet'])\n","test_labels=list(df['label'])\n","test_labels=list(pd.get_dummies(test_labels,drop_first=True)['real'])\n","# df.shape\n","\n","\n","# train_labels\n","\n","# from sklearn.model_selection import train_test_split\n","# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state = 0)\n","\n","# !pip install transformers[torch]\n","\n","# Load model directly\n","from transformers import AutoTokenizer, AutoModelForSequenceClassification\n","\n","tokenizer = AutoTokenizer.from_pretrained(\"roberta-base\")\n","\n","from transformers import DistilBertTokenizerFast\n","# tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')\n","\n","# from transformers import DistilBertTokenizerFast\n","# tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')\n","\n","train_encodings = tokenizer(train_texts, truncation=True, padding=True, max_length=512)\n","val_encodings = tokenizer(val_texts, truncation=True, padding=True, max_length=512)\n","test_encodings = tokenizer(test_texts, truncation=True, padding=True, max_length=512)\n","\n","# print(len(val_texts))\n","# print(val_texts)\n","# print(type(val_encodings))\n","# print(val_encodings)\n","\n","import torch\n","\n","class IMDbDataset(torch.utils.data.Dataset):\n","    def __init__(self, encodings, labels):\n","        self.encodings = encodings\n","        self.labels = labels\n","\n","    def __getitem__(self, idx):\n","        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n","        item['labels'] = torch.tensor(self.labels[idx])\n","        return item\n","\n","    def __len__(self):\n","        return len(self.labels)\n","\n","train_dataset = IMDbDataset(train_encodings, train_labels)\n","val_dataset = IMDbDataset(val_encodings, val_labels)\n","test_dataset = IMDbDataset(test_encodings, test_labels)\n","\n","# train_dataset[5]\n","\n","from transformers import Trainer, TrainingArguments, BertForSequenceClassification\n","\n","training_args = TrainingArguments(\n","    output_dir='./results',          # output directory\n","    num_train_epochs=1,              # total number of training epochs\n","    per_device_train_batch_size=1,  # batch size per device during training\n","    per_device_eval_batch_size=1,   # batch size for evaluation\n","    warmup_steps=500,                # number of warmup steps for learning rate scheduler\n","    weight_decay=0.01,               # strength of weight decay\n","    logging_dir='./logs',            # directory for storing logs\n","    logging_steps=10,\n","    gradient_accumulation_steps=1\n",")\n","\n","model = AutoModelForSequenceClassification.from_pretrained(\"roberta-base\")\n","# model = BertForSequenceClassification.from_pretrained(\"sarkerlab/SocBERT-base\")\n","\n","trainer = Trainer(\n","    model=model,                         # the instantiated ðŸ¤— Transformers model to be trained\n","    args=training_args,                  # training arguments, defined above\n","    train_dataset=train_dataset,         # training dataset\n","    eval_dataset=val_dataset             # evaluation dataset\n",")\n","\n","trainer.train()\n","\n","trainer.evaluate(test_dataset)\n","\n","\n","\n","trainer.predict(test_dataset)\n","\n","trainer.predict(test_dataset)[1].shape\n","\n","output=trainer.predict(test_dataset)[1]\n","\n","from sklearn.metrics import confusion_matrix\n","\n","cm=confusion_matrix(test_labels, output)\n","# cm\n","\n","trainer.save_model('RoBERTa_model')\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":445},"id":"Txfdw8dHW0zS","executionInfo":{"status":"ok","timestamp":1698648853926,"user_tz":-330,"elapsed":26400,"user":{"displayName":"Prakhar Singh","userId":"07600268745645597584"}},"outputId":"0af047cc-5cbe-4d4f-9510-d6ccd729e436"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stderr","text":["Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [100/100 00:08, Epoch 1/1]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>10</td>\n","      <td>0.754300</td>\n","    </tr>\n","    <tr>\n","      <td>20</td>\n","      <td>0.745900</td>\n","    </tr>\n","    <tr>\n","      <td>30</td>\n","      <td>0.773500</td>\n","    </tr>\n","    <tr>\n","      <td>40</td>\n","      <td>0.672300</td>\n","    </tr>\n","    <tr>\n","      <td>50</td>\n","      <td>0.692400</td>\n","    </tr>\n","    <tr>\n","      <td>60</td>\n","      <td>0.759700</td>\n","    </tr>\n","    <tr>\n","      <td>70</td>\n","      <td>0.679700</td>\n","    </tr>\n","    <tr>\n","      <td>80</td>\n","      <td>0.713000</td>\n","    </tr>\n","    <tr>\n","      <td>90</td>\n","      <td>0.663700</td>\n","    </tr>\n","    <tr>\n","      <td>100</td>\n","      <td>0.676400</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[]},"metadata":{}}]},{"cell_type":"code","metadata":{"id":"awPXefiYqQsF","executionInfo":{"status":"ok","timestamp":1698648552436,"user_tz":-330,"elapsed":2,"user":{"displayName":"Prakhar Singh","userId":"07600268745645597584"}}},"source":["import pandas as pd\n","df = pd.read_csv('/content/Constraint_Train.csv', encoding='ISO-8859-1')\n","df = df.head(100)\n","train_texts=list(df['tweet'])\n","train_labels=list(df['label'])\n","\n","df = pd.read_csv('/content/Constraint_Val.csv', encoding='ISO-8859-1')\n","df = df.head(30)\n","val_texts=list(df['tweet'])\n","val_labels=list(df['label'])\n","\n","df = pd.read_csv('/content/Constraint_test.csv', encoding='ISO-8859-1')\n","df = df.head(30)\n","test_texts=list(df['tweet'])\n","test_labels=list(df['label'])\n","# df.shape"],"execution_count":14,"outputs":[]},{"cell_type":"code","metadata":{"id":"aZpBn3MtssZL","executionInfo":{"status":"ok","timestamp":1698648554168,"user_tz":-330,"elapsed":2,"user":{"displayName":"Prakhar Singh","userId":"07600268745645597584"}}},"source":["train_labels=list(pd.get_dummies(train_labels,drop_first=True)['real'])\n","val_labels=list(pd.get_dummies(val_labels,drop_first=True)['real'])\n","test_labels=list(pd.get_dummies(test_labels,drop_first=True)['real'])"],"execution_count":15,"outputs":[]},{"cell_type":"code","source":["# train_labels"],"metadata":{"id":"5_kL8nAH9yyo"},"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"dLFDWda0rIKw"},"source":["# from sklearn.model_selection import train_test_split\n","# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state = 0)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"AqOBGiGErZgj","outputId":"f0abc62d-17b5-4644-f4b1-775f75f7ef18","executionInfo":{"status":"ok","timestamp":1698648821265,"user_tz":-330,"elapsed":5771,"user":{"displayName":"Prakhar Singh","userId":"07600268745645597584"}}},"source":["!pip install transformers[torch]"],"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: transformers[torch] in /usr/local/lib/python3.10/dist-packages (4.34.1)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (3.12.4)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.17.3)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (1.23.5)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (23.2)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (6.0.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2023.6.3)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2.31.0)\n","Requirement already satisfied: tokenizers<0.15,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.14.1)\n","Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.4.0)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (4.66.1)\n","Requirement already satisfied: torch!=1.12.0,>=1.10 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2.1.0+cu118)\n","Requirement already satisfied: accelerate>=0.20.3 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.24.0)\n","Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.20.3->transformers[torch]) (5.9.5)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers[torch]) (2023.6.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers[torch]) (4.5.0)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch!=1.12.0,>=1.10->transformers[torch]) (1.12)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch!=1.12.0,>=1.10->transformers[torch]) (3.2)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch!=1.12.0,>=1.10->transformers[torch]) (3.1.2)\n","Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch!=1.12.0,>=1.10->transformers[torch]) (2.1.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (3.3.1)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (3.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (2023.7.22)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch!=1.12.0,>=1.10->transformers[torch]) (2.1.3)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch!=1.12.0,>=1.10->transformers[torch]) (1.3.0)\n"]}]},{"cell_type":"code","source":["# Load model directly\n","from transformers import AutoTokenizer, AutoModelForSequenceClassification\n","\n","tokenizer = AutoTokenizer.from_pretrained(\"roberta-base\")\n","\n","from transformers import DistilBertTokenizerFast\n","# tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')"],"metadata":{"id":"mNQvaKL3Bpnv","executionInfo":{"status":"ok","timestamp":1698648563093,"user_tz":-330,"elapsed":945,"user":{"displayName":"Prakhar Singh","userId":"07600268745645597584"}}},"execution_count":17,"outputs":[]},{"cell_type":"code","metadata":{"id":"bcNEJ6perOSs"},"source":["# from transformers import DistilBertTokenizerFast\n","# tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-OL3fgLvrXvH","executionInfo":{"status":"ok","timestamp":1698648563093,"user_tz":-330,"elapsed":2,"user":{"displayName":"Prakhar Singh","userId":"07600268745645597584"}}},"source":["train_encodings = tokenizer(train_texts, truncation=True, padding=True, max_length=512)\n","val_encodings = tokenizer(val_texts, truncation=True, padding=True, max_length=512)\n","test_encodings = tokenizer(test_texts, truncation=True, padding=True, max_length=512)"],"execution_count":18,"outputs":[]},{"cell_type":"code","source":["print(len(val_texts))\n","print(val_texts)\n","print(type(val_encodings))\n","print(val_encodings)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9T1XZOLWusUS","executionInfo":{"status":"ok","timestamp":1698577583924,"user_tz":-330,"elapsed":7,"user":{"displayName":"Prakhar Singh","userId":"07600268745645597584"}},"outputId":"b7db5e6e-b5b5-4f63-9c99-d806f0d9cc0f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["30\n","['Chinese converting to Islam after realising that no muslim was affected by #Coronavirus #COVD19 in the country', '11 out of 13 people (from the Diamond Princess Cruise ship) who had intially tested negative in tests in Japan were later confirmed to be positive in the United States.', 'COVID-19 Is Caused By A Bacterium, Not Virus And Can Be Treated With Aspirin', \"Mike Pence in RNC speech praises Donald TrumpÃ¢\\x80\\x99s COVID-19 Ã¢\\x80\\x9cseamlessÃ¢\\x80\\x9d partnership with governors and leaves out the president's state feuds: https://t.co/qJ6hSewtgB #RNC2020 https://t.co/OFoeRZDfyY\", \"6/10 Sky's @EdConwaySky explains the latest #COVID19 data and government announcement. Get more on the #coronavirus data hereÃ°\\x9f\\x91\\x87 https://t.co/jvGZlSbFjH https://t.co/PygSKXesBg\", 'No one can leave managed isolation for any reason without returning a negative test. If they refuse a test they can then be held for a period of up to 28 days. Ã¢\\x81Â£ Ã¢\\x81Â£ On June the 16th exemptions on compassionate grounds have been suspended. Ã¢\\x81Â£ Ã¢\\x81Â£', '#IndiaFightsCorona India has one of the lowest #COVID19 mortality globally with less than 2% Case Fatality Rate. As a result of supervised home isolation &amp; effective clinical treatment many States/UTs have CFR lower than the national average. https://t.co/QLiK8YPP7E', 'RT @WHO: #COVID19 transmission occurs primarily through direct indirect or close contact with infected people through their saliva and resÃ¢\\x80Â¦', 'News and media outlet ABP Majha on the basis of an internal memo of South Central Railway reported that a special train has been announced to take the stranded migrant workers home.', '???Church services can???t resume until we???re all vaccinated, says Bill Gates.??Ã¯Â¿Â½', 'You can still fly the friendly skies without fear of COVID if airlines stay serious about safety. https://t.co/H8mGjwDl7G #coronavirus', 'India records yet another single-day rise of over 28000 new cases while more than 5.5 lakh individuals have recovered from COVID-19. Kerala government sets up its first plasma bank in the state following in the steps of Delhi and West Bengal. #COVID19 #CoronavirusFacts https://t.co/JhSQUqMvta', 'A conspiracy theory audio about #COVID19 testing in #India circulating on @WhatsApp allegedly from MLA Geeta Jain ( @connectGEETA ). We do a quick #FactCheck on this to find that the minister has already clarified on same. https://t.co/SBhSTSr1MH', 'Tomorrow April 6 we will pass 10000 coronavirus deaths. We passed 5000 on April 2. We passed 1000 on March 26. We passed 100 on March 18.', 'Labour\\'s @Keir_Starmer asks about problems with #COVID19 testing adding that \"pretending there isn\\'t a problem is part of the problem\". The PM says testing is \"at a record high\" with the UK \"testing more people than any other European country\". #PMQs: https://t.co/EjtlULskwu https://t.co/gqX6NijZmh', 'Youth Sports Organizations: Consider implementing CDCÃ¢\\x80\\x99s new tips for maintaining healthy operations during the #COVID19 outbreak such as staggered schedules &amp; options for individuals at higher risk for severe illness. More tips: https://t.co/tnPX5fnVG6. https://t.co/L31ep4cmUw', \"Last note: Washington DC's total test count fell by ~22% presumably pulling out antibody tests.\", 'Gov. Andrew Cuomo Ã¢\\x80\\x9cwas simply saying if we can share 20 percent of your excess your non-used ventilators to help people in other parts of the state on a voluntary basis that would be great. Of course there was a reaction to that which was not positive.\"', '???The Democrats are pushing for an implanted microchip in humans, and everyone to be vaccinated.??Ã¯Â¿Â½', 'Arizona and Missouri report only facilities with outbreaks not actual cases deaths or facility namesÃ¢\\x80\\x94this appears transparent but doesnÃ¢\\x80\\x99t convey COVID-19Ã¢\\x80\\x99s true impact. County health depts (Maricopa AZ and St. Louis MO) provide both statesÃ¢\\x80\\x99 most reliable source of LTC data.', 'Mike Pence introduces program to cure coronavirus carriers with conversion therapy https://t.co/A36KAO2NWa https://t.co/bp0SDO25F0', '#IndiaFightsCorona Nearly 74% of the New Recovered cases are found in 10 States/UTs. Maharashtra has maintained this lead with 19476 cases (22.3%) for the sixth consecutive day. https://t.co/ksnkELo2gf', 'Advisory issued by the state police of Telangana (India) instructing people to be vigilant about the possibility of increase in thefts due to the COVID-19 crisis in the country.', \"Our Weekly Epidemiological Report #NCDCWER provides updates on epidemic-prone diseases in Nigeria This week's editorial focuses on the scale-up of #COVID19 testing in all 36 states &amp; FCT through the national testing strategy Read via: https://t.co/lhenjls99o https://t.co/muFsr7JrQO\", 'As at 08:00 pm 2nd April there are 184 confirmed cases 20 discharged 2 deaths For a breakdown of cases by states- https://t.co/zQrpNeOfet Currently; Lagos- 98 FCT- 38 Osun- 14 Oyo- 8 Akwa Ibom- 5 Ogun- 4 Edo- 4 Kaduna- 4 Bauchi- 3 Enugu- 2 Ekiti- 2 Rivers-1 Benue- 1', 'Breathlessness excessive fatigue and muscle aches from COVID can last for months. https://t.co/OUhBRirKpE', 'TodayÃ¢\\x80\\x99s second confirmed case is a 59 year old woman who travelled from Delhi and who arrived in Auckland on 15 June. She was tested while at the Grand Millennium managed isolation facility and was travelling with her partner who has also been tested and whose result is pending.', 'Parent Makes Impassioned Plea To Coronavirus https://t.co/E8AHhknapG #kids #facebook #coronavirus', 'Everyone can help prevent spread of #COVID19. Clara the #Coronavirus Self-Checker can help you decide when to call your doctor if you are feeling sick. Start using Clara here: https://t.co/5FnxlOcZpu. https://t.co/dYNwHAgEQi', 'As of June 16 more than 2.1 million #COVID19 cases have been reported in the U.S. with 37 states and jurisdictions reporting more than 10000 cases. See how many cases have been reported in your state: https://t.co/wiuFBKR3Uh. https://t.co/jeB60wmCxl']\n","<class 'transformers.tokenization_utils_base.BatchEncoding'>\n","{'input_ids': [[101, 2822, 16401, 2000, 7025, 2044, 27504, 2008, 2053, 5152, 2001, 5360, 2011, 1001, 21887, 23350, 1001, 2522, 16872, 16147, 1999, 1996, 2406, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [101, 2340, 2041, 1997, 2410, 2111, 1006, 2013, 1996, 6323, 4615, 8592, 2911, 1007, 2040, 2018, 20014, 4818, 2135, 7718, 4997, 1999, 5852, 1999, 2900, 2020, 2101, 4484, 2000, 2022, 3893, 1999, 1996, 2142, 2163, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [101, 2522, 17258, 1011, 2539, 2003, 3303, 2011, 1037, 24024, 1010, 2025, 7865, 1998, 2064, 2022, 5845, 2007, 2004, 8197, 6657, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [101, 3505, 7279, 3401, 1999, 29300, 2278, 4613, 27128, 6221, 8398, 3022, 2522, 17258, 1011, 2539, 2004, 5243, 19968, 7971, 2050, 5386, 2007, 11141, 1998, 3727, 2041, 1996, 2343, 1005, 1055, 2110, 13552, 2015, 1024, 16770, 1024, 1013, 1013, 1056, 1012, 2522, 1013, 1053, 3501, 2575, 7898, 7974, 2102, 18259, 1001, 29300, 2278, 11387, 11387, 16770, 1024, 1013, 1013, 1056, 1012, 2522, 1013, 1997, 8913, 15378, 20952, 2100, 2100, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [101, 1020, 1013, 2184, 3712, 1005, 1055, 1030, 3968, 8663, 14035, 4801, 7607, 1996, 6745, 1001, 2522, 17258, 16147, 2951, 1998, 2231, 8874, 1012, 2131, 2062, 2006, 1996, 1001, 21887, 23350, 2951, 2182, 29668, 16770, 1024, 1013, 1013, 1056, 1012, 2522, 1013, 1046, 2615, 2290, 2480, 4877, 29292, 3501, 2232, 16770, 1024, 1013, 1013, 1056, 1012, 2522, 1013, 1052, 2100, 5620, 2243, 20156, 2497, 2290, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [101, 2053, 2028, 2064, 2681, 3266, 12477, 2005, 2151, 3114, 2302, 4192, 1037, 4997, 3231, 1012, 2065, 2027, 10214, 1037, 3231, 2027, 2064, 2059, 2022, 2218, 2005, 1037, 2558, 1997, 2039, 2000, 2654, 2420, 1012, 1037, 29646, 1037, 29646, 2006, 2238, 1996, 5767, 19621, 2015, 2006, 29353, 5286, 2031, 2042, 6731, 1012, 1037, 29646, 1037, 29646, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [101, 1001, 2634, 20450, 9363, 26788, 2634, 2038, 2028, 1997, 1996, 7290, 1001, 2522, 17258, 16147, 13356, 16452, 2007, 2625, 2084, 1016, 1003, 2553, 10611, 3012, 3446, 1012, 2004, 1037, 2765, 1997, 13588, 2188, 12477, 1004, 23713, 1025, 4621, 6612, 3949, 2116, 2163, 1013, 21183, 2015, 2031, 12935, 2099, 2896, 2084, 1996, 2120, 2779, 1012, 16770, 1024, 1013, 1013, 1056, 1012, 2522, 1013, 1053, 18393, 2620, 22571, 2361, 2581, 2063, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [101, 19387, 1030, 2040, 1024, 1001, 2522, 17258, 16147, 6726, 5158, 3952, 2083, 3622, 14958, 2030, 2485, 3967, 2007, 10372, 2111, 2083, 2037, 26308, 1998, 24501, 2050, 29649, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [101, 2739, 1998, 2865, 13307, 11113, 2361, 16686, 3270, 2006, 1996, 3978, 1997, 2019, 4722, 24443, 1997, 2148, 2430, 2737, 2988, 2008, 1037, 2569, 3345, 2038, 2042, 2623, 2000, 2202, 1996, 15577, 20731, 3667, 2188, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [101, 1029, 1029, 1029, 2277, 2578, 2064, 1029, 1029, 1029, 1056, 13746, 2127, 2057, 1029, 1029, 1029, 2128, 2035, 12436, 14693, 23854, 1010, 2758, 3021, 6733, 1012, 1029, 1029, 1045, 1094, 1092, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [101, 2017, 2064, 2145, 4875, 1996, 5379, 15717, 2302, 3571, 1997, 2522, 17258, 2065, 7608, 2994, 3809, 2055, 3808, 1012, 16770, 1024, 1013, 1013, 1056, 1012, 2522, 1013, 1044, 2620, 24798, 3501, 21724, 2140, 2581, 2290, 1001, 21887, 23350, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [101, 2634, 2636, 2664, 2178, 2309, 1011, 2154, 4125, 1997, 2058, 13427, 8889, 2047, 3572, 2096, 2062, 2084, 1019, 1012, 1019, 2474, 10023, 3633, 2031, 6757, 2013, 2522, 17258, 1011, 2539, 1012, 8935, 2231, 4520, 2039, 2049, 2034, 12123, 2924, 1999, 1996, 2110, 2206, 1999, 1996, 4084, 1997, 6768, 1998, 2225, 8191, 1012, 1001, 2522, 17258, 16147, 1001, 21887, 23350, 7011, 16649, 16770, 1024, 1013, 1013, 1056, 1012, 2522, 1013, 1046, 7898, 28940, 4160, 2213, 2615, 2696, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [101, 1037, 9714, 3399, 5746, 2055, 1001, 2522, 17258, 16147, 5604, 1999, 1001, 2634, 22458, 2006, 1030, 2054, 3736, 9397, 9382, 2013, 18619, 20277, 2696, 17136, 1006, 1030, 7532, 18372, 2696, 1007, 1012, 2057, 2079, 1037, 4248, 1001, 2755, 5403, 3600, 2006, 2023, 2000, 2424, 2008, 1996, 2704, 2038, 2525, 20485, 2006, 2168, 1012, 16770, 1024, 1013, 1013, 1056, 1012, 2522, 1013, 24829, 7898, 3215, 2099, 2487, 2213, 2232, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [101, 4826, 2258, 1020, 2057, 2097, 3413, 6694, 2692, 21887, 23350, 6677, 1012, 2057, 2979, 13509, 2006, 2258, 1016, 1012, 2057, 2979, 6694, 2006, 2233, 2656, 1012, 2057, 2979, 2531, 2006, 2233, 2324, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [101, 4428, 1005, 1055, 1030, 26679, 2099, 1035, 2732, 5017, 5176, 2055, 3471, 2007, 1001, 2522, 17258, 16147, 5604, 5815, 2008, 1000, 12097, 2045, 3475, 1005, 1056, 1037, 3291, 2003, 2112, 1997, 1996, 3291, 1000, 1012, 1996, 7610, 2758, 5604, 2003, 1000, 2012, 1037, 2501, 2152, 1000, 2007, 1996, 2866, 1000, 5604, 2062, 2111, 2084, 2151, 2060, 2647, 2406, 1000, 1012, 1001, 7610, 4160, 2015, 1024, 16770, 1024, 1013, 1013, 1056, 1012, 2522, 1013, 1041, 3501, 19646, 28426, 2243, 16050, 16770, 1024, 1013, 1013, 1056, 1012, 2522, 1013, 1043, 4160, 2595, 2575, 3490, 3501, 2480, 2213, 2232, 102], [101, 3360, 2998, 4411, 1024, 5136, 14972, 26629, 3022, 2047, 10247, 2005, 8498, 7965, 3136, 2076, 1996, 1001, 2522, 17258, 16147, 8293, 2107, 2004, 14648, 20283, 1004, 23713, 1025, 7047, 2005, 3633, 2012, 3020, 3891, 2005, 5729, 7355, 1012, 2062, 10247, 1024, 16770, 1024, 1013, 1013, 1056, 1012, 2522, 1013, 28286, 2361, 2595, 2629, 2546, 2078, 2615, 2290, 2575, 1012, 16770, 1024, 1013, 1013, 1056, 1012, 2522, 1013, 1048, 21486, 13699, 2549, 27487, 25974, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [101, 2197, 3602, 1024, 2899, 5887, 1005, 1055, 2561, 3231, 4175, 3062, 2011, 1066, 2570, 1003, 10712, 4815, 2041, 27781, 5852, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [101, 18079, 1012, 4080, 12731, 19506, 22091, 3022, 3432, 3038, 2065, 2057, 2064, 3745, 2322, 3867, 1997, 2115, 9987, 2115, 2512, 1011, 2109, 18834, 11733, 6591, 2000, 2393, 2111, 1999, 2060, 3033, 1997, 1996, 2110, 2006, 1037, 10758, 3978, 2008, 2052, 2022, 2307, 1012, 1997, 2607, 2045, 2001, 1037, 4668, 2000, 2008, 2029, 2001, 2025, 3893, 1012, 1000, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [101, 1029, 1029, 1029, 1996, 8037, 2024, 6183, 2005, 2019, 27159, 2098, 12702, 5428, 2361, 1999, 4286, 1010, 1998, 3071, 2000, 2022, 12436, 14693, 23854, 1012, 1029, 1029, 1045, 1094, 1092, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [101, 5334, 1998, 5284, 3189, 2069, 4128, 2007, 8293, 2015, 2025, 5025, 3572, 6677, 2030, 4322, 3415, 25457, 2015, 3544, 13338, 2021, 2987, 4017, 16636, 2522, 17258, 1011, 2539, 3022, 2995, 4254, 1012, 2221, 2740, 29466, 2015, 1006, 16266, 3597, 4502, 17207, 1998, 2358, 1012, 3434, 9587, 1007, 3073, 2119, 2163, 2050, 2087, 10539, 3120, 1997, 8318, 2278, 2951, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [101, 3505, 7279, 3401, 13999, 2565, 2000, 9526, 21887, 23350, 11363, 2007, 7584, 7242, 16770, 1024, 1013, 1013, 1056, 1012, 2522, 1013, 1037, 21619, 2912, 2080, 2475, 2078, 4213, 16770, 1024, 1013, 1013, 1056, 1012, 2522, 1013, 17531, 16223, 3527, 17788, 2546, 2692, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [101, 1001, 2634, 20450, 9363, 26788, 3053, 6356, 1003, 1997, 1996, 2047, 6757, 3572, 2024, 2179, 1999, 2184, 2163, 1013, 21183, 2015, 1012, 12434, 2038, 5224, 2023, 2599, 2007, 4006, 2575, 3572, 1006, 2570, 1012, 1017, 1003, 1007, 2005, 1996, 4369, 5486, 2154, 1012, 16770, 1024, 1013, 1013, 1056, 1012, 2522, 1013, 29535, 8950, 18349, 2475, 25708, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [101, 7319, 3843, 2011, 1996, 2110, 2610, 1997, 23764, 1006, 2634, 1007, 16021, 26310, 2111, 2000, 2022, 6819, 20142, 4630, 2055, 1996, 6061, 1997, 3623, 1999, 11933, 2015, 2349, 2000, 1996, 2522, 17258, 1011, 2539, 5325, 1999, 1996, 2406, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [101, 2256, 4882, 4958, 5178, 4328, 10091, 3189, 1001, 13316, 16409, 13777, 3640, 14409, 2006, 16311, 1011, 13047, 7870, 1999, 7387, 2023, 2733, 1005, 1055, 8368, 7679, 2006, 1996, 4094, 1011, 2039, 1997, 1001, 2522, 17258, 16147, 5604, 1999, 2035, 4029, 2163, 1004, 23713, 1025, 4429, 2102, 2083, 1996, 2120, 5604, 5656, 3191, 3081, 1024, 16770, 1024, 1013, 1013, 1056, 1012, 2522, 1013, 1048, 10222, 3501, 4877, 2683, 2683, 2080, 16770, 1024, 1013, 1013, 1056, 1012, 2522, 1013, 14163, 10343, 2099, 2581, 3501, 2099, 4160, 2080, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [101, 2004, 2012, 5511, 1024, 4002, 7610, 3416, 2258, 2045, 2024, 19681, 4484, 3572, 2322, 14374, 1016, 6677, 2005, 1037, 12554, 1997, 3572, 2011, 2163, 1011, 16770, 1024, 1013, 1013, 1056, 1012, 2522, 1013, 1062, 4160, 14536, 23585, 7959, 2102, 2747, 1025, 16738, 1011, 5818, 4429, 2102, 1011, 4229, 9808, 4609, 1011, 2403, 1051, 7677, 1011, 1022, 17712, 4213, 21307, 5358, 1011, 1019, 13958, 4609, 1011, 1018, 18314, 1011, 1018, 10556, 27584, 2050, 1011, 1018, 8670, 15217, 1011, 1017, 4372, 15916, 2226, 1011, 1016, 23174, 3775, 1011, 1016, 5485, 1011, 1015, 3841, 5657, 1011, 1015, 102, 0, 0], [101, 16701, 2791, 11664, 16342, 1998, 6740, 12336, 2015, 2013, 2522, 17258, 2064, 2197, 2005, 2706, 1012, 16770, 1024, 1013, 1013, 1056, 1012, 2522, 1013, 15068, 2232, 23736, 8024, 5051, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [101, 2651, 3022, 2117, 4484, 2553, 2003, 1037, 5354, 2095, 2214, 2450, 2040, 7837, 2013, 6768, 1998, 2040, 3369, 1999, 8666, 2006, 2321, 2238, 1012, 2016, 2001, 7718, 2096, 2012, 1996, 2882, 10144, 3266, 12477, 4322, 1998, 2001, 8932, 2007, 2014, 4256, 2040, 2038, 2036, 2042, 7718, 1998, 3005, 2765, 2003, 14223, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [101, 6687, 3084, 17727, 12054, 19798, 14865, 2000, 21887, 23350, 16770, 1024, 1013, 1013, 1056, 1012, 2522, 1013, 1041, 2620, 4430, 2232, 2243, 2532, 26952, 1001, 4268, 1001, 9130, 1001, 21887, 23350, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [101, 3071, 2064, 2393, 4652, 3659, 1997, 1001, 2522, 17258, 16147, 1012, 10254, 1996, 1001, 21887, 23350, 2969, 1011, 4638, 2121, 2064, 2393, 2017, 5630, 2043, 2000, 2655, 2115, 3460, 2065, 2017, 2024, 3110, 5305, 1012, 2707, 2478, 10254, 2182, 1024, 16770, 1024, 1013, 1013, 1056, 1012, 2522, 1013, 1019, 2546, 26807, 4135, 27966, 14289, 1012, 16770, 1024, 1013, 1013, 1056, 1012, 2522, 1013, 1040, 6038, 2860, 3270, 3351, 14702, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [101, 2004, 1997, 2238, 2385, 2062, 2084, 1016, 1012, 1015, 2454, 1001, 2522, 17258, 16147, 3572, 2031, 2042, 2988, 1999, 1996, 1057, 1012, 1055, 1012, 2007, 4261, 2163, 1998, 17370, 7316, 2062, 2084, 6694, 2692, 3572, 1012, 2156, 2129, 2116, 3572, 2031, 2042, 2988, 1999, 2115, 2110, 1024, 16770, 1024, 1013, 1013, 1056, 1012, 2522, 1013, 15536, 16093, 2497, 21638, 2509, 27225, 1012, 16770, 1024, 1013, 1013, 1056, 1012, 2522, 1013, 15333, 2497, 16086, 2860, 12458, 2595, 2140, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]}\n"]}]},{"cell_type":"code","source":["import torch\n","\n","class IMDbDataset(torch.utils.data.Dataset):\n","    def __init__(self, encodings, labels):\n","        self.encodings = encodings\n","        self.labels = labels\n","\n","    def __getitem__(self, idx):\n","        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n","        item['labels'] = torch.tensor(self.labels[idx])\n","        return item\n","\n","    def __len__(self):\n","        return len(self.labels)\n","\n","train_dataset = IMDbDataset(train_encodings, train_labels)\n","val_dataset = IMDbDataset(val_encodings, val_labels)\n","test_dataset = IMDbDataset(test_encodings, test_labels)"],"metadata":{"id":"yMbVTrmsHGYC","executionInfo":{"status":"ok","timestamp":1698648563094,"user_tz":-330,"elapsed":3,"user":{"displayName":"Prakhar Singh","userId":"07600268745645597584"}}},"execution_count":19,"outputs":[]},{"cell_type":"code","source":["train_dataset[5]"],"metadata":{"id":"DBj-glm8NhdL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from transformers import Trainer, TrainingArguments, BertForSequenceClassification\n","\n","training_args = TrainingArguments(\n","    output_dir='./results',          # output directory\n","    num_train_epochs=1,              # total number of training epochs\n","    per_device_train_batch_size=1,  # batch size per device during training\n","    per_device_eval_batch_size=1,   # batch size for evaluation\n","    warmup_steps=1,                # number of warmup steps for learning rate scheduler\n","    weight_decay=0.01,               # strength of weight decay\n","    logging_dir='./logs',            # directory for storing logs\n","    logging_steps=1\n",")"],"metadata":{"id":"ywxFvnwZHOH6","executionInfo":{"status":"ok","timestamp":1698648567318,"user_tz":-330,"elapsed":2,"user":{"displayName":"Prakhar Singh","userId":"07600268745645597584"}}},"execution_count":20,"outputs":[]},{"cell_type":"code","source":["model = AutoModelForSequenceClassification.from_pretrained(\"roberta-base\")\n","# model = BertForSequenceClassification.from_pretrained(\"sarkerlab/SocBERT-base\")\n","\n","trainer = Trainer(\n","    model=model,                         # the instantiated ðŸ¤— Transformers model to be trained\n","    args=training_args,                  # training arguments, defined above\n","    train_dataset=train_dataset,         # training dataset\n","    eval_dataset=val_dataset             # evaluation dataset\n",")\n","\n"],"metadata":{"id":"wBxgMOhuHYFl","executionInfo":{"status":"ok","timestamp":1698648571611,"user_tz":-330,"elapsed":2103,"user":{"displayName":"Prakhar Singh","userId":"07600268745645597584"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"bd0c989a-50b3-4dfd-e139-06d7737c4e88"},"execution_count":21,"outputs":[{"output_type":"stream","name":"stderr","text":["Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]}]},{"cell_type":"code","source":["trainer.train()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"40F4M1a6qJEf","executionInfo":{"status":"ok","timestamp":1698648587292,"user_tz":-330,"elapsed":12915,"user":{"displayName":"Prakhar Singh","userId":"07600268745645597584"}},"outputId":"ac56e5db-8704-41b5-e413-83660b452de7"},"execution_count":22,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [100/100 00:12, Epoch 1/1]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.775300</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>0.600700</td>\n","    </tr>\n","    <tr>\n","      <td>3</td>\n","      <td>0.792500</td>\n","    </tr>\n","    <tr>\n","      <td>4</td>\n","      <td>0.763500</td>\n","    </tr>\n","    <tr>\n","      <td>5</td>\n","      <td>0.735200</td>\n","    </tr>\n","    <tr>\n","      <td>6</td>\n","      <td>0.763000</td>\n","    </tr>\n","    <tr>\n","      <td>7</td>\n","      <td>0.500700</td>\n","    </tr>\n","    <tr>\n","      <td>8</td>\n","      <td>0.559400</td>\n","    </tr>\n","    <tr>\n","      <td>9</td>\n","      <td>0.540900</td>\n","    </tr>\n","    <tr>\n","      <td>10</td>\n","      <td>0.499900</td>\n","    </tr>\n","    <tr>\n","      <td>11</td>\n","      <td>1.036000</td>\n","    </tr>\n","    <tr>\n","      <td>12</td>\n","      <td>0.444400</td>\n","    </tr>\n","    <tr>\n","      <td>13</td>\n","      <td>1.266100</td>\n","    </tr>\n","    <tr>\n","      <td>14</td>\n","      <td>0.166400</td>\n","    </tr>\n","    <tr>\n","      <td>15</td>\n","      <td>0.267200</td>\n","    </tr>\n","    <tr>\n","      <td>16</td>\n","      <td>0.216100</td>\n","    </tr>\n","    <tr>\n","      <td>17</td>\n","      <td>0.094300</td>\n","    </tr>\n","    <tr>\n","      <td>18</td>\n","      <td>0.046800</td>\n","    </tr>\n","    <tr>\n","      <td>19</td>\n","      <td>0.027800</td>\n","    </tr>\n","    <tr>\n","      <td>20</td>\n","      <td>3.561200</td>\n","    </tr>\n","    <tr>\n","      <td>21</td>\n","      <td>0.023400</td>\n","    </tr>\n","    <tr>\n","      <td>22</td>\n","      <td>3.041700</td>\n","    </tr>\n","    <tr>\n","      <td>23</td>\n","      <td>0.079000</td>\n","    </tr>\n","    <tr>\n","      <td>24</td>\n","      <td>0.018400</td>\n","    </tr>\n","    <tr>\n","      <td>25</td>\n","      <td>0.011100</td>\n","    </tr>\n","    <tr>\n","      <td>26</td>\n","      <td>0.020900</td>\n","    </tr>\n","    <tr>\n","      <td>27</td>\n","      <td>0.022900</td>\n","    </tr>\n","    <tr>\n","      <td>28</td>\n","      <td>5.523700</td>\n","    </tr>\n","    <tr>\n","      <td>29</td>\n","      <td>0.004100</td>\n","    </tr>\n","    <tr>\n","      <td>30</td>\n","      <td>0.008700</td>\n","    </tr>\n","    <tr>\n","      <td>31</td>\n","      <td>5.691000</td>\n","    </tr>\n","    <tr>\n","      <td>32</td>\n","      <td>0.007300</td>\n","    </tr>\n","    <tr>\n","      <td>33</td>\n","      <td>4.100700</td>\n","    </tr>\n","    <tr>\n","      <td>34</td>\n","      <td>3.777400</td>\n","    </tr>\n","    <tr>\n","      <td>35</td>\n","      <td>4.133000</td>\n","    </tr>\n","    <tr>\n","      <td>36</td>\n","      <td>3.048100</td>\n","    </tr>\n","    <tr>\n","      <td>37</td>\n","      <td>2.233100</td>\n","    </tr>\n","    <tr>\n","      <td>38</td>\n","      <td>1.808400</td>\n","    </tr>\n","    <tr>\n","      <td>39</td>\n","      <td>0.308300</td>\n","    </tr>\n","    <tr>\n","      <td>40</td>\n","      <td>0.303500</td>\n","    </tr>\n","    <tr>\n","      <td>41</td>\n","      <td>1.441700</td>\n","    </tr>\n","    <tr>\n","      <td>42</td>\n","      <td>0.282300</td>\n","    </tr>\n","    <tr>\n","      <td>43</td>\n","      <td>0.292700</td>\n","    </tr>\n","    <tr>\n","      <td>44</td>\n","      <td>1.288900</td>\n","    </tr>\n","    <tr>\n","      <td>45</td>\n","      <td>0.255100</td>\n","    </tr>\n","    <tr>\n","      <td>46</td>\n","      <td>1.304600</td>\n","    </tr>\n","    <tr>\n","      <td>47</td>\n","      <td>2.686400</td>\n","    </tr>\n","    <tr>\n","      <td>48</td>\n","      <td>1.636200</td>\n","    </tr>\n","    <tr>\n","      <td>49</td>\n","      <td>0.326900</td>\n","    </tr>\n","    <tr>\n","      <td>50</td>\n","      <td>0.308700</td>\n","    </tr>\n","    <tr>\n","      <td>51</td>\n","      <td>1.074600</td>\n","    </tr>\n","    <tr>\n","      <td>52</td>\n","      <td>0.348000</td>\n","    </tr>\n","    <tr>\n","      <td>53</td>\n","      <td>1.204800</td>\n","    </tr>\n","    <tr>\n","      <td>54</td>\n","      <td>0.310500</td>\n","    </tr>\n","    <tr>\n","      <td>55</td>\n","      <td>0.137600</td>\n","    </tr>\n","    <tr>\n","      <td>56</td>\n","      <td>2.332900</td>\n","    </tr>\n","    <tr>\n","      <td>57</td>\n","      <td>1.411300</td>\n","    </tr>\n","    <tr>\n","      <td>58</td>\n","      <td>0.272200</td>\n","    </tr>\n","    <tr>\n","      <td>59</td>\n","      <td>0.137200</td>\n","    </tr>\n","    <tr>\n","      <td>60</td>\n","      <td>0.183900</td>\n","    </tr>\n","    <tr>\n","      <td>61</td>\n","      <td>0.160700</td>\n","    </tr>\n","    <tr>\n","      <td>62</td>\n","      <td>1.979600</td>\n","    </tr>\n","    <tr>\n","      <td>63</td>\n","      <td>0.092300</td>\n","    </tr>\n","    <tr>\n","      <td>64</td>\n","      <td>1.805900</td>\n","    </tr>\n","    <tr>\n","      <td>65</td>\n","      <td>2.895100</td>\n","    </tr>\n","    <tr>\n","      <td>66</td>\n","      <td>3.194800</td>\n","    </tr>\n","    <tr>\n","      <td>67</td>\n","      <td>3.444800</td>\n","    </tr>\n","    <tr>\n","      <td>68</td>\n","      <td>0.050100</td>\n","    </tr>\n","    <tr>\n","      <td>69</td>\n","      <td>0.070800</td>\n","    </tr>\n","    <tr>\n","      <td>70</td>\n","      <td>0.068200</td>\n","    </tr>\n","    <tr>\n","      <td>71</td>\n","      <td>0.040500</td>\n","    </tr>\n","    <tr>\n","      <td>72</td>\n","      <td>1.628900</td>\n","    </tr>\n","    <tr>\n","      <td>73</td>\n","      <td>0.035900</td>\n","    </tr>\n","    <tr>\n","      <td>74</td>\n","      <td>0.026700</td>\n","    </tr>\n","    <tr>\n","      <td>75</td>\n","      <td>0.041200</td>\n","    </tr>\n","    <tr>\n","      <td>76</td>\n","      <td>0.034600</td>\n","    </tr>\n","    <tr>\n","      <td>77</td>\n","      <td>0.022700</td>\n","    </tr>\n","    <tr>\n","      <td>78</td>\n","      <td>1.649000</td>\n","    </tr>\n","    <tr>\n","      <td>79</td>\n","      <td>1.886200</td>\n","    </tr>\n","    <tr>\n","      <td>80</td>\n","      <td>2.258100</td>\n","    </tr>\n","    <tr>\n","      <td>81</td>\n","      <td>3.943900</td>\n","    </tr>\n","    <tr>\n","      <td>82</td>\n","      <td>2.274800</td>\n","    </tr>\n","    <tr>\n","      <td>83</td>\n","      <td>1.502500</td>\n","    </tr>\n","    <tr>\n","      <td>84</td>\n","      <td>0.011300</td>\n","    </tr>\n","    <tr>\n","      <td>85</td>\n","      <td>0.014900</td>\n","    </tr>\n","    <tr>\n","      <td>86</td>\n","      <td>2.862100</td>\n","    </tr>\n","    <tr>\n","      <td>87</td>\n","      <td>0.049700</td>\n","    </tr>\n","    <tr>\n","      <td>88</td>\n","      <td>1.330900</td>\n","    </tr>\n","    <tr>\n","      <td>89</td>\n","      <td>0.048700</td>\n","    </tr>\n","    <tr>\n","      <td>90</td>\n","      <td>1.102700</td>\n","    </tr>\n","    <tr>\n","      <td>91</td>\n","      <td>0.039400</td>\n","    </tr>\n","    <tr>\n","      <td>92</td>\n","      <td>0.882800</td>\n","    </tr>\n","    <tr>\n","      <td>93</td>\n","      <td>0.046100</td>\n","    </tr>\n","    <tr>\n","      <td>94</td>\n","      <td>1.155400</td>\n","    </tr>\n","    <tr>\n","      <td>95</td>\n","      <td>0.640000</td>\n","    </tr>\n","    <tr>\n","      <td>96</td>\n","      <td>0.021800</td>\n","    </tr>\n","    <tr>\n","      <td>97</td>\n","      <td>0.011200</td>\n","    </tr>\n","    <tr>\n","      <td>98</td>\n","      <td>0.004300</td>\n","    </tr>\n","    <tr>\n","      <td>99</td>\n","      <td>0.007800</td>\n","    </tr>\n","    <tr>\n","      <td>100</td>\n","      <td>0.293800</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"execute_result","data":{"text/plain":["TrainOutput(global_step=100, training_loss=1.0268887466657908, metrics={'train_runtime': 12.8194, 'train_samples_per_second': 7.801, 'train_steps_per_second': 7.801, 'total_flos': 7554165066000.0, 'train_loss': 1.0268887466657908, 'epoch': 1.0})"]},"metadata":{},"execution_count":22}]},{"cell_type":"code","metadata":{"id":"R534aDi3xD0s","colab":{"base_uri":"https://localhost:8080/","height":127},"executionInfo":{"status":"ok","timestamp":1698648593340,"user_tz":-330,"elapsed":1527,"user":{"displayName":"Prakhar Singh","userId":"07600268745645597584"}},"outputId":"cd91c902-6b6f-4a95-9dc5-42bb1c45d647"},"source":["trainer.evaluate(test_dataset)"],"execution_count":23,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='30' max='30' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [30/30 00:01]\n","    </div>\n","    "]},"metadata":{}},{"output_type":"execute_result","data":{"text/plain":["{'eval_loss': 0.6234884858131409,\n"," 'eval_runtime': 1.1327,\n"," 'eval_samples_per_second': 26.486,\n"," 'eval_steps_per_second': 26.486,\n"," 'epoch': 1.0}"]},"metadata":{},"execution_count":23}]},{"cell_type":"code","metadata":{"id":"HvP_ktb-xyFq"},"source":[],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"UyBmI1WcxKjG","colab":{"base_uri":"https://localhost:8080/","height":614},"executionInfo":{"status":"ok","timestamp":1698648597372,"user_tz":-330,"elapsed":1954,"user":{"displayName":"Prakhar Singh","userId":"07600268745645597584"}},"outputId":"2e0962ff-7ce0-4999-fb7f-6e9a3ce24fa5"},"source":["trainer.predict(test_dataset)"],"execution_count":24,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[]},"metadata":{}},{"output_type":"execute_result","data":{"text/plain":["PredictionOutput(predictions=array([[-2.5559509 ,  2.7681262 ],\n","       [ 0.04195629, -0.08116461],\n","       [ 0.0421141 ,  0.04737667],\n","       [-2.324264  ,  2.613964  ],\n","       [-0.88124394,  1.0040797 ],\n","       [-0.94549954,  1.0451622 ],\n","       [-1.8400902 ,  2.1112416 ],\n","       [-0.11342328,  0.16150379],\n","       [-2.6947258 ,  2.8709815 ],\n","       [-2.2441635 ,  2.4187236 ],\n","       [-1.1391364 ,  1.3310229 ],\n","       [ 0.06187572, -0.04798014],\n","       [-1.8842508 ,  2.0843213 ],\n","       [-0.32520664,  0.4541872 ],\n","       [ 0.10788678, -0.08128266],\n","       [-0.9861113 ,  1.1583264 ],\n","       [-0.32898915,  0.38364446],\n","       [ 0.01330426,  0.0626809 ],\n","       [-2.6568034 ,  2.8255754 ],\n","       [-1.4920697 ,  1.7068912 ],\n","       [-0.96929836,  1.1031146 ],\n","       [ 0.16256465, -0.12915666],\n","       [-1.8373505 ,  2.0207405 ],\n","       [-0.2926962 ,  0.3299022 ],\n","       [-1.7737709 ,  1.9972073 ],\n","       [-1.4281087 ,  1.5330293 ],\n","       [-0.03927647,  0.04861167],\n","       [-0.9862964 ,  1.069712  ],\n","       [-1.9407202 ,  2.109009  ],\n","       [ 0.14722742, -0.1143083 ]], dtype=float32), label_ids=array([1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0,\n","       1, 0, 1, 1, 0, 0, 1, 0]), metrics={'test_loss': 0.6234884858131409, 'test_runtime': 1.9377, 'test_samples_per_second': 15.482, 'test_steps_per_second': 15.482})"]},"metadata":{},"execution_count":24}]},{"cell_type":"code","metadata":{"id":"9Qc5FtM8xn9A","colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"status":"ok","timestamp":1698648605970,"user_tz":-330,"elapsed":854,"user":{"displayName":"Prakhar Singh","userId":"07600268745645597584"}},"outputId":"700cb662-bc17-463a-9711-a1938dcf7511"},"source":["trainer.predict(test_dataset)[1].shape"],"execution_count":27,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[]},"metadata":{}},{"output_type":"execute_result","data":{"text/plain":["(30,)"]},"metadata":{},"execution_count":27}]},{"cell_type":"code","metadata":{"id":"fUVX_IhWxkxg","colab":{"base_uri":"https://localhost:8080/","height":17},"executionInfo":{"status":"ok","timestamp":1698648608638,"user_tz":-330,"elapsed":971,"user":{"displayName":"Prakhar Singh","userId":"07600268745645597584"}},"outputId":"cad6b7ea-6f12-46e0-96c3-2993123eb392"},"source":["output=trainer.predict(test_dataset)[1]"],"execution_count":28,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[]},"metadata":{}}]},{"cell_type":"code","metadata":{"id":"cfCE06jQu5cI","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1698648609649,"user_tz":-330,"elapsed":5,"user":{"displayName":"Prakhar Singh","userId":"07600268745645597584"}},"outputId":"fecbbd50-6e98-4911-9486-0a8f2329f6bd"},"source":["from sklearn.metrics import confusion_matrix\n","\n","cm=confusion_matrix(test_labels, output)\n","cm"],"execution_count":29,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[15,  0],\n","       [ 0, 15]])"]},"metadata":{},"execution_count":29}]},{"cell_type":"code","metadata":{"id":"okD5we1NwhQW","executionInfo":{"status":"ok","timestamp":1698648612948,"user_tz":-330,"elapsed":1473,"user":{"displayName":"Prakhar Singh","userId":"07600268745645597584"}}},"source":["trainer.save_model('senti_model')"],"execution_count":30,"outputs":[]},{"cell_type":"code","metadata":{"id":"f9MbFGrEyNTS"},"source":[],"execution_count":null,"outputs":[]}]}